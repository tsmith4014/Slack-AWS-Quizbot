{
    "1. Your developers want to run fully provisioned EC2 instances to support their application code deployments but prefer not to have to worry about manually configuring and launching the necessary infrastructure. Which of the following should they use?\n   1. AWS Lambda\n   2. AWS Elastic Beanstalk\n   3. Amazon EC2 Auto Scaling\n   4. Amazon Route 53": "2. Elastic Beanstalk takes care of the ongoing underlying deployment details for you, allowing you to focus exclusively on your code. Lambda will respond to trigger events by running code a single time, Auto Scaling will ramp up existing infrastructure in response to demand, and Route 53 manages DNS and network routing.",
    "2. Some of your application's end users are complaining of delays when accessing your resources from remote geographic locations. Which of these services would be the most likely to help reduce the delays?\n   1. Amazon CloudFront\n   2. Amazon Route 53\n   3. Elastic Load Balancing\n   4. Amazon Glacier": "1. CloudFront maintains a network of endpoints where cached versions of your application data are stored to provide quicker responses to user requests. Route 53 manages DNS and network routing, Elastic Load Balancing routes incoming user requests among a cluster of available servers, and Glacier provides high-latency, low-cost file storage.",
    "3. Which of the following is the best use-case scenario for Elastic Block Store?\n   1. You need a cheap and reliable place to store files your application can access.\n   2. You need a safe place to store backup archives from your local servers.\n   3. You need a source for on-demand compute cycles to meet fluctuating demand for your application.\n   4. You need persistent storage for the filesystem run by your EC2 instance.": "4. Elastic Block Store provides virtual block devices (think: storage drives) on which you can install and run filesystems and data operations. It is not normally a cost-effective option for long-term data storage.",
    "4. You need to integrate your company's local user access controls with some of your AWS resources. Which of the following can help you control the way your local users access your AWS services and administration console? (Choose two.)\n   1. AWS Identity and Access Management (IAM)\n   2. Key Management Service (KMS)\n   3. AWS Directory Service\n   4. Simple WorkFlow (SWF)\n   5. Amazon Cognito": "1, 3. AWS IAM lets you create user accounts, groups, and roles and assign them rights and permissions over specific services and resources within your AWS account. Directory Service allows you to integrate your resources with external users and resources through third-party authentication services. KMS is a tool for generating and managing encryption keys, and SWF is a tool for coordinating application tasks. Amazon Cognito can be used to manage authentication for your application users, but not your internal admin teams.",
    "5. The data consumed by the application you're planning will require more speed and flexibility than you can get from a closely defined relational database structure. Which AWS database service should you choose?\n   1. Relational Database Service (RDS)\n   2. Amazon Aurora\n   3. Amazon DynamoDB\n   4. Key Management Service (KMS)": "3. DynamoDB provides a NoSQL (nonrelational) database service. Both are good for workloads that can be more efficiently run without the relational schema of SQL database engines (like those, including Aurora, that are offered by RDS). KMS is a tool for generating and managing encryption keys.",
    "6. You've launched an EC2 application server instance in the AWS Ireland region and you need to access it from the web. Which of the following is the correct endpoint address that you should use?\n   1. compute.eu-central-1.amazonaws.com\n   2. ec2.eu-central-1.amazonaws.com\n   3. elasticcomputecloud.eu-west-2.amazonaws.com\n   4. ec2.eu-west-1.amazonaws.com": "4. EC2 endpoints will always start with an ec2 prefix followed by the region designation (eu-west-1 in the case of Ireland).",
    "7. When working to set up your first AWS deployment, you keep coming across the term availability zone. What exactly is an availability zone?\n   1. An isolated physical datacenter within an AWS region\n   2. A region containing multiple isolated datacenters\n   3. A single network subnet used by resources within a single region\n   4. A single isolated server room within a datacenter": "1. An availability zone is an isolated physical datacenter within an AWS region. Regions are geographic areas that contain multiple availability zones, subnets are IP address blocks that can be used within a zone to organize your networked resources, and there can be multiple datacenters within an availability zone.",
    "8. As you plan your multi-tiered, multi-instance AWS application, you need a way to effectively organize your instances and configure their network connectivity and access control. Which tool will let you do that?\n   1. Load Balancing\n   2. Amazon Virtual Private Cloud (VPC)\n   3. Amazon CloudFront\n   4. AWS endpoints": "2. VPCs are virtualized network environments where you can control the connectivity of your EC2 (and RDS, etc.) infrastructure. Load Balancing routes incoming user requests among a cluster of available servers, CloudFront maintains a network of endpoints where cached versions of your application data are stored to provide quicker responses to user requests, and AWS endpoints are URIs that point to AWS resources within your account.",
    "9. You want to be sure that the application you're building using EC2 and S3 resources will be reliable enough to meet the regulatory standards required within your industry. What should you check?\n   1. Historical uptime log records\n   2. The AWS Program Compliance Tool\n   3. The AWS service level agreement (SLA)\n   4. The AWS Compliance Programs documentation page\n   5. The AWS Shared Responsibility Model": "3. The AWS service level agreement tells you the level of service availability you can realistically expect from a particular AWS service. You can use this information when assessing your compliance with external standards. Log records, though they can offer important historical performance metrics, probably won't be enough to prove compliance. The AWS Compliance Programs page will show you only which regulatory programs can be satisfied with AWS resources, not whether a particular configuration will meet their demands. The AWS Shared Responsibility Model outlines who is responsible for various elements of your AWS infrastructure. There is no AWS Program Compliance tool.",
    "10. Your organization's operations team members need a way to access and administer your AWS infrastructure via your local command line or shell scripts. Which of the following tools will let them do that?\n   1. AWS Config\n   2. AWS CLI\n   3. AWS SDK\n   4. The AWS Console": "2. The AWS Command-Line Interface (CLI) is a tool for accessing AWS APIs from the command-line shell of your local computer. The AWS SDK is for accessing resources programmatically, the AWS Console works graphically through your browser, and AWS Config is a service for editing and auditing your AWS account resources.",
    "11. While building a large AWS-based application, your company has been facing configuration problems they can't solve on their own. As a result, they need direct access to AWS support for both development and IT team leaders. Which support plan should you purchase?\n   1. Business\n   2. Developer\n   3. Basic\n   4. Enterprise": "1. Unlike the Basic and Developer plans (which allow access to a support associate to no or one user, respectively), the Business plan allows multiple team members.",
    "12. You've got a complex, multi-tiered application running on local servers that you want to migrate to the cloud. Which of these tools will provide you with the specific tools you'll need to move the application with the least risk and the least disruption?\n   1. AWS Application Migration Service\n   2. AWS Migration Hub\n   3. AWS Application Discovery Service\n   4. AWS Lift and Shift": "1. Application Migration Service can automate the testing and transfer of AWS-bound migrations of your non-cloud application servers. That, therefore, is the correct answer. Migration Hub is a high-level tools for coordinating migrations. Application Discovery Service takes an inventory of your infrastructure but doesn't migrate anything itself. Lift and Shift doesn't actually exist, but don't you wish it did.",
    "1. You need to deploy multiple EC2 Linux instances that will provide your company with virtual private networks (VPNs) using software called OpenVPN. Which of the following will be the most efficient solutions? (Choose two.)\n   1. Select a regular Linux AMI and bootstrap it using user data that will install and configure the OpenVPN package on the instance and use it for your VPN instances.\n   2. Search the community AMIs for an official AMI provided and supported by the OpenVPN company.\n   3. Search the AWS Marketplace to see whether there's an official AMI provided and supported by the OpenVPN company.\n   4. Select a regular Linux AMI and SSH to manually install and configure the OpenVPN package.\n   5. Create a site-to-site VPN connection from the wizard in the AWS VPC dashboard.": "1, 3. Many third-party companies maintain official and supported AMIs running their software on the AWS Marketplace. AMIs hosted among the community AMIs are not always official and supported versions. Since your company will need several such instances, you'll be better off automating the process by bootstrapping rather than having to configure the software manually each time. The site-to-site VPN tool doesn't use OpenVPN.",
    "2. As part of your company's long-term cloud migration strategy, you have a VMware virtual machine in your local infrastructure that you'd like to copy to your AWS account and run as an EC2 instance. Which of the following will be necessary steps? (Choose two.)\n   1. Import the virtual machine to your AWS region using a secure SSH tunnel.\n   2. Import the virtual machine using VM Import/Export.\n   3. Select the imported VM from among your private AMIs and launch an instance.\n   4. Select the imported VM from the AWS Marketplace AMIs and launch an instance.\n   5. Use the AWS CLI to securely copy your virtual machine image to an S3 bucket within the AWS region you'll be using.": "2, 3. The VM Import/Export tool handles the secure and reliable transfer for a virtual machine between your AWS account and local datacenter. A successfully imported VM will appear among the private AMIs in the region you selected. Direct S3 uploads and SSH tunnels are not associated with VM Import/Export.",
    "3. Your AWS CLI command to launch an AMI as an EC2 instance has failed, giving you an error message that includes InvalidAMIID.NotFound. What of the following is the most likely cause?\n   1. You haven't properly configured the ~/.aws/config file.\n   2. The AMI is being updated and is temporarily unavailable.\n   3. Your key pair file has been given the wrong (overly permissive) permissions.\n   4. The AMI you specified exists in a different region than the one you've currently specified.": "4. AMIs are specific to a single AWS region and cannot be deployed into any other region. If your AWS CLI or its key pair was not configured properly, your connection would have failed completely. A public AMI being unavailable because it's \u201cupdating\u201d is theoretically possible but unlikely.",
    "4. The sensitivity of the data your company works with means that the instances you run must be secured through complete physical isolation. What should you specify as you configure a new instance?\n   1. Dedicated Host tenancy\n   2. Shared tenancy\n   3. Dedicated Instance tenancy\n   4. Isolated tenancy": "1. Only Dedicated Host tenancy offers full isolation. Shared tenancy instances will often share hardware with operations belonging to other organizations. Dedicated instance tenancy instances may be hosted on the same physical server as other instances within your account.",
    "5. Normally, two instances running m5.large instance types can handle the traffic accessing your online e-commerce site, but you know that you will face short, unpredictable periods of high demand. Which of the following choices should you implement? (Choose two.)\n   1. Configure autoscaling.\n   2. Configure load balancing.\n   3. Purchase two m5.large instances on the spot market and as many on-demand instances as necessary.\n   4. Shut down your m5.large instances and purchase instances using a more robust instance type to replace them.\n   5. Purchase two m5.large reserve instances and as many on-demand instances as necessary.": "1, 5. Reserve instances will give you the best price for instances you know will be running 24/7, whereas on-demand makes the most sense for workloads that will run at unpredictable times but can't be shut down until they're no longer needed. Load balancing controls traffic routing and, on its own, has no impact on your ability to meet changing demand. Since the m5.large instance type is all you need to meet normal workloads, you'll be wasting money by running a larger type 24/7.",
    "6. Which of the following use cases would be most cost effective if run using spot market instances?\n   1. Your e-commerce website is built using a publicly available AMI.\n   2. You provide high-end video rendering services using a fault-tolerant process that can easily manage a job that was unexpectedly interrupted.\n   3. You're running a backend database that must be reliably updated to keep track of critical transactions.\n   4. Your deployment runs as a static website on S3.": "2. Spot market instances can be shut down with only a minimal (two-minute) warning, so they're not recommended for workloads that require reliably predictable service. Even if your AMI can be relaunched, the interrupted workload will still be lost. Static S3 websites don't run on EC2 infrastructure in the first place.",
    "7. In the course of a routine infrastructure audit, your organization discovers that some of your running EC2 instances are not configured properly and must be updated. Which of the following configuration details cannot be changed on an existing EC2 instance?\n   1. AMI\n   2. Instance type\n   3. Security group\n   4. Public IP address": "1. You can edit or even add or remove security groups from running instances and the changes will take effect instantly. Similarly, you can associate or release an elastic IP address to/from a running instance. You can change an instance type as long as you shut down the instance first. But the AMI can't be changed; you'll need to create an entirely new instance.",
    "8. For an account with multiple resources running as part of multiple projects, which of the following key/value combination examples would make for the most effective identification convention for resource tags?\n   1. servers:server1\n   2. project1:server1\n   3. EC2:project1:server1\n   4. server1:project1": "2. The first of two (and not three) strings in a resource tag is the key\u2014the group to which the specific resource belongs. The second string is the value, which identifies the resource itself. If the key looks too much like the value, it can cause confusion.",
    "9. Which of the following EBS options will you need to keep your data-hungry application that requires up to 20,000 IOPS happy?\n   1. Cold HDD\n   2. General-purpose SSD\n   3. Throughput-optimized HDD\n   4. Provisioned-IOPS SSD": "4. Provisioned-IOPS SSD volumes are currently the only type that comes close to 20,000 IOPS. In fact, under the right circumstances, they can deliver up to 256,000 IOPS.",
    "10. Your organization needs to introduce Auto Scaling to its infrastructure and needs to generate a \u201cgolden image\u201d AMI from an existing EBS volume. This image will need to be shared among multiple AWS accounts belonging to your organization. Which of the following steps will get you there? (Choose three.)\n   1. Create an image from a detached EBS volume, use it to create a snapshot, select your new AMI from your private collection, and use it for your launch configuration.\n   2. Create a snapshot of the EBS root volume you need, use it to create an image, select your new AMI from your private collection, and use it for your launch configuration.\n   3. Create an image from the EBS volume attached to the instance, select your new AMI from your private collection, and use it for your launch configuration.\n   4. Search the AWS Marketplace for the appropriate image and use it for your launch configuration.\n   5. Import the snapshot of an EBS root volume from a different AWS account, use it to create an image, select your new AMI from your private collection, and use it for your launch configuration.": "2, 3, 5. Options B, C, and E are steps necessary for creating and sharing such an image. When an image is created, a snapshot is automatically created from which an AMI is built. You do not, however, create a snapshot from an image. The AWS Marketplace contains only public images; hopefully, no one will have uploaded your organization's private image there!",
    "11. Which of the following are benefits of instance store volumes? (Choose two.)\n   1. Instance volumes are physically attached to the server that's hosting your instance, allowing faster data access.\n   2. Instance volumes can be used to store data even after the instance is shut down.\n   3. The use of instance volumes does not incur costs (beyond those for the instance itself).\n   4. You can set termination protection so that an instance volume can't be accidentally shut down.\n   5. Instance volumes are commonly used as a base for the creation of AMIs.": "1, 3. The fact that instance volumes are physically attached to the host server and add nothing to an instance cost is a benefit. The data on instance volumes is ephemeral and will be lost as soon as the instance is shut down. There is no way to set termination protection for instance volumes because they're dependent on the life cycle of their host instances.",
    "12. According to default behavior (and AWS recommendations), which of the following IP addresses could be assigned as the private IP for an EC2 instance? (Choose two.)\n   1. 54.61.211.98\n   2. 23.176.92.3\n   3. 172.17.23.43\n   4. 10.0.32.176\n   5. 192.140.2.118": "3, 4. By default, EC2 uses the standard address blocks for private subnets, so all private addresses will fall within these ranges: 10.0.0.0 to 10.255.255.255, 172.16.0.0 to 172.31.255.255, and 192.168.0.0 to 192.168.255.255.",
    "13. You need to restrict access to your EC2 instance-based application to only certain clients and only certain targets. Which three attributes of an incoming data packet are used by a security group to determine whether it should be allowed through? (Choose three.)\n   1. Network port\n   2. Source address\n   3. Datagram header size\n   4. Network protocol\n   5. Destination address": "1, 2, 4. Ports and source and destinations addresses are considered by security group rules. Security group rules do not take packet size into consideration. Since a security group is directly associated with specific objects, there's no need to reference the target address.",
    "14. How are IAM roles commonly used to ensure secure resource access in relation to EC2 instances?\n   1. A role can assign processes running on the EC2 instance itself permission to access other AWS resources.\n   2. A user can be given permission to authenticate as a role and access all associated resources.\n   3. A role can be associated with individual instance-based processes (Linux instances only), giving them permission to access other AWS resources.\n   4. A role can give users and resources permission to access the EC2 instance.": "4. IAM roles define how resources access other resources. Users cannot authenticate as an instance role, nor can a role be associated with an instance's internal system process.",
    "15. You have an instance running within a private subnet that needs external network access to receive software updates and patches. Which of the following can securely provide that access from a public subnet within the same VPC? (Choose two.)\n   1. Internet gateway\n   2. NAT instance\n   3. Virtual private gateway\n   4. NAT gateway\n   5. VPN": "2, 4. NAT instances and NAT gateways are AWS tools for safely routing traffic between private and public subnets and from there, out to the Internet. An Internet gateway connects a VPC with the Internet, and a virtual private gateway connects a VPC with a remote site over a secure VPN. A stand-alone VPN wouldn't normally be helpful for this purpose.",
    "16. What do you have to do to securely authenticate to the GUI console of a Windows EC2 session?\n   1. Use the private key of your key pair to initiate an SSH tunnel session.\n   2. Use the public key of your key pair to initiate an SSH tunnel session.\n   3. Use the public key of your key pair to retrieve the password you'll use to log in.\n   4. Use the private key of your key pair to retrieve the password you'll use to log in.": "4. The client computer in an encrypted operation must always use the private key to authenticate. For EC2 instances running Windows, you retrieve the password you'll use for the GUI login using your private key.",
    "17. Your application deployment includes multiple EC2 instances that need low-latency connections to each other. Which of the following AWS tools will allow you to locate EC2 instances closer to each other to reduce network latency?\n   1. Load balancing\n   2. Placement groups\n   3. AWS Systems Manager\n   4. AWS Fargate": "2. Placement groups allow you to specify where your EC2 instances will live. Load balancing directs external user requests between multiple EC2 instances, Systems Manager provides tools for monitoring and managing your resources, and Fargate is an interface for administering Docker containers on Amazon ECS.",
    "18. To save configuration time and money, you want your application to run only when network events trigger it but shut down immediately after. Which of the following will do that for you?\n   1. AWS Lambda\n   2. AWS Elastic Beanstalk\n   3. Amazon Elastic Container Service (ECS)\n   4. Auto Scaling": "1. Lambda can be used as such a trigger. Beanstalk launches and manages infrastructure for your application that will remain running until you manually stop it, ECS manages Docker containers but doesn't necessarily stop them when a task is done, and Auto Scaling can add instances to an already running deployment to meet demand.",
    "19. Which of the following will allow you to quickly copy a virtual machine image from your local infrastructure to your AWS VPC?\n   1. AWS Simple Storage Service (S3)\n   2. AWS Snowball\n   3. VM Import/Export\n   4. AWS Direct Connect": "3. VM Import/Export will do this. S3 buckets are used to store an image, but they're not directly involved in the import operation. Snowball is a physical high-capacity storage device that Amazon ships to your office for you to load data and ship back. Direct Connect uses Amazon partner providers to build a high-speed connection between your servers and your AWS VPC.",
    "20. You've configured an EC2 Auto Scaling group to use a launch configuration to provision and install an application on several instances. You now need to reconfigure Auto Scaling to install an additional application on new instances. Which of the following should you do?\n   1. Modify the launch configuration.\n   2. Create a launch template and configure the Auto Scaling group to use it.\n   3. Modify the launch template.\n   4. Modify the CloudFormation template.": "2. You can modify a launch template by creating a new version of it; however, the question indicates that the Auto Scaling group was created using a launch configuration. You can't modify a launch configuration. Auto Scaling doesn't use CloudFormation templates.",
    "21. You create an Auto Scaling group with a minimum group size of 3, a maximum group size of 10, and a desired capacity of 5. You then manually terminate two instances in the group. Which of the following will Auto Scaling do?\n   1. Create two new instances.\n   2. Reduce the desired capacity to 3.\n   3. Nothing.\n   4. Increment the minimum group size to 5.": "1. Auto Scaling strives to maintain the number of instances specified in the desired capacity setting. If the desired capacity setting isn't set, Auto Scaling will attempt to maintain the number of instances specified by the minimum group size. Given a desired capacity value of 5, there should be five healthy instances. If you manually terminate two of them, Auto Scaling will create two new ones to replace them. Auto Scaling will not adjust the desired capacity or minimum group size.",
    "22. You're running an application that receives a spike in traffic on the first day of every month. You want to configure Auto Scaling to add more instances before the spike begins and then add additional instances in proportion to the CPU utilization of each instance. Which of the following should you implement? (Choose all that apply.)\n   1. Target tracking policies\n   2. Scheduled actions\n   3. Step scaling policies\n   4. Simple scaling policies\n   5. Load balancing": "2, 3. Scheduled actions can adjust the minimum and maximum group sizes and the desired capacity on a schedule, which is useful when your application has a predictable load pattern. To add more instances in proportion to the aggregate CPU utilization of the group, implement step scaling policies. Target tracking policies adjust the desired capacity of a group to keep the threshold of a given metric near a predefined value. Simple scaling policies simply add more instances when a defined CloudWatch alarm triggers, but the number of instances added is not proportional to the value of the metric.",
    "23. As part of your new data backup protocols, you need to manually take EBS snapshots of several hundred volumes. Which type of Systems Manager document enables you to do this?\n   1. Command\n   2. Automation\n   3. Policy\n   4. Manual": "2. Automation documents let you perform actions against your AWS resources, including taking EBS snapshots. Although called automation documents, you can still manually execute them. A command document performs actions within a Linux or a Windows instance. A policy document works only with State Manager and can't take an EBS snapshot. There's no manual document type.",
    "24. You want to launch and manage a complex microservices container workload in AWS but you want to avoid as many configuration headaches as possible, You figure you'll be fine with whatever defaults you're offered. Which of these platforms is your best choice?\n   1. Amazon Elastic Kubernetes Service\n   2. AWS Fargate\n   3. Amazon EKS Distro\n   4. Amazon Elastic Container Service": "2. Fargate is a service that uses either ECS or EKS infrastructure under the hood, but actually abstracts away most of the configuration details. Therefore, Fargate is your best bet. EKS and ECS give you far greater control over your configuration but, as a result, are more complex. EKS Distro is a way of running K8s containers in your own infrastructure and, if anything, is the most complex option of all.",
    "1. Your organization runs Linux-based EC2 instances that all require low-latency read/write access to a single set of files. Which of the following AWS services are your best choices? (Choose two.)\n   1. AWS Storage Gateway\n   2. AWS S3\n   3. Amazon Elastic File System\n   4. AWS Elastic Block Store": "1, 3. Storage Gateway and EFS provide the required read/write access. S3 can be used to share files, but it doesn't offer low-latency access\u2014and its eventual consistency won't work well with filesystems. EBS volumes can be used only for a single instance at a time.",
    "2. Your organization expects to be storing and processing large volumes of data in many small increments. When considering S3 usability, you'll need to know whether you'll face any practical limitations in the use of AWS account resources. Which of the following will normally be available only in limited amounts?\n   1. PUT requests/month against an S3 bucket\n   2. The volume of data space available per S3 bucket\n   3. Account-wide S3 storage space\n   4. The number of S3 buckets within a single account": "4. In theory, at least, there's no limit to the data you can upload to a single bucket or to all the buckets in your account or to the number of times you upload (using the PUT command). By default, however, you are allowed only 100 S3 buckets per account.",
    "3. You have a publicly available file called filename stored in an S3 bucket named bucketname. Which of the following addresses will successfully retrieve the file using a web browser?\n   1. s3.amazonaws.com/bucketname/filename\n   2. filename/bucketname.s3.amazonaws.com\n   3. s3://bucketname/filename\n   4. s3://filename/bucketname": "1. HTTP (web) requests must address the s3.amazonaws.com domain along with the bucket and filenames.",
    "4. If you want the files stored in an S3 bucket to be accessible using a familiar directory hierarchy system, you'll need to specify prefixes and delimiters. What are prefixes and delimiters?\n   1. A prefix is the name common to the objects you want to group, and a delimiter is the bar character (|).\n   2. A prefix is the DNS name that precedes the amazonaws.com domain, and a delimiter is the name you want to give your file directory.\n   3. A prefix is the name common to the objects you want to group, and a delimiter is a forward slash character (/).\n   4. A prefix is the name common to the file type you want to identify, and a delimiter is a forward slash character (/).": "3. A prefix is the name common to the objects you want to group, and a slash character (/) can be used as a delimiter. The bar character (|) would be treated as part of the name rather than as a delimiter. Although DNS names can have prefixes, they're not the same as prefixes in S3.",
    "5. Your web application relies on data objects stored in AWS S3 buckets. Compliance with industry regulations requires that those objects be encrypted and that related events be closely tracked. Which combination of tools should you use? (Choose two.)\n   1. Server-side encryption\n   2. Amazon S3-Managed Keys\n   3. AWS KMS-Managed Keys\n   4. Client-side encryption\n   5. AWS End-to-End managed keys": "1, 3. Client-side encryption occurs before an object reaches the bucket (i.e., before it comes to rest in the bucket). Only AWS KMS-Managed Keys provide an audit trail. AWS End-to-End managed keys don\u2019t exist as an AWS service.",
    "6. You are engaged in a deep audit of the use of your AWS resources and you need to better understand the structure and content of your S3 server access logs. Which of the following operational details are likely to be included in S3 server access logs? (Choose three.)\n   1. Source bucket name\n   2. Action requested\n   3. Current bucket size\n   4. API bucket creation calls\n   5. Response status": "1, 2, 5. S3 server access logs don't report the source bucket's current size. They don't track API calls\u2014that's something covered by AWS CloudTrail.",
    "7. You're assessing the level of durability you'll need to sufficiently ensure the long-term viability of a new web application you're planning. Which of the following risks are covered by S3's data durability guarantees? (Choose two.)\n   1. User misconfiguration\n   2. Account security breach\n   3. Infrastructure failure\n   4. Temporary service outages\n   5. Datacenter security breach": "3, 5. The S3 guarantee only covers the physical infrastructure owned by AWS. Temporary service outages are related to \u201cavailability\u201d and not \u201cdurability.\u201d",
    "8. Which of the following explains the difference in durability between S3'sStandard-IA and S3 Intelligent-Tiering classes?\n   1. Standard-IA data has only 99.9% availability, whereas Intelligent-Tiering's availability depends on the data's current state.\n   2. Standard-IA data is heavily replicated but only within a single availability zone, whereas Intelligent-Tiering data is only lightly replicated.\n   3. Standard-IA data is replicated across AWS regions, whereas Intelligent-Tiering data is restricted to a single region.\n   4. Standard-IA data is automatically backed up to Amazon Glacier, whereas Intelligent-Tiering data remains within S3.": "1. Standard-IA data has only a 99.9% availability rate, whereas the availability (and other features) of Intelligent-Tiering data will change across its life cycle.",
    "9. Which of the following is the 12-month availability guarantee for the S3 Standard-IA class?\n   1. 99.99 percent\n   2. 99.9 percent\n   3. 99.999999999 percent\n   4. 99.5 percent": "2. The S3 Standard-IA (Infrequent Access) class is guaranteed to be available 99.9 percent of the time.",
    "10. Your application regularly writes data to an S3 bucket, but you're worried about the potential for data corruption as a result of conflicting concurrent operations. Which of the following data operations would not be subject to concerns about eventual consistency?\n   1. Operations immediately preceding the deletion of an existing object\n   2. Operations subsequent to the updating of an existing object\n   3. Operations subsequent to the deletion of an existing object\n   4. Operations subsequent to the creation of a new object": "4. S3 can't guarantee instant consistency across their infrastructure for changes to existing objects, but there aren't such concerns for newly created objects.",
    "11. You're worried that updates to the important data you store in S3 might incorrectly overwrite existing files. What must you do to protect objects in S3 buckets from being accidentally lost?\n   1. Nothing. S3 protects existing files by default.\n   2. Nothing. S3 saves older versions of your files by default.\n   3. Enable versioning.\n   4. Enable file overwrite protection.": "3. Object versioning must be manually enabled for each object to prevent older versions of the object from being deleted.",
    "12. Your S3 buckets contain many thousands of objects. Some of them could be moved to less expensive storage classes and others still require instant availability. How can you apply transitions between storage classes for only certain objects within an S3 bucket?\n   1. By specifying particular prefixes when you define your life cycle rules.\n   2. This isn't possible. Life cycle rules must apply to all the objects in a bucket.\n   3. By specifying particular prefixes when you create the bucket.\n   4. By importing a predefined life cycle rule template.": "1. S3 life cycle rules can incorporate specifying objects by prefix. There's no such thing as a life cycle template.",
    "13. Which of the following classes will usually make the most sense for long-term storage when included within a sequence of life cycle rules?\n   1. S3 Glacier Flexible Retrieval\n   2. Reduced Redundancy\n   3. S3 One Zone-IA\n   4. S3 Standard-IA": "1. S3 Glacier offers the least expensive and most highly resilient storage within the AWS ecosystem. Reduced Redundancy is not resilient and, in any case, is no longer recommended. S3 One Zone and S3 Standard are relatively expensive.",
    "14. Which of the following are the recommended methods for providing secure and controlled access to your buckets? (Choose two.)\n   1. S3 access control lists (ACLs)\n   2. S3 bucket policies\n   3. IAM policies\n   4. Security groups\n   5. AWS Key Management Service": "2, 3. ACLs are a legacy feature that isn't as flexible as IAM or S3 bucket polices. Security groups are not used with S3 buckets. KMS is an encryption key management tool and isn't used for authentication.",
    "15. In the context of an S3 bucket policy, which of the following statements describes a principal?\n   1. The AWS service being defined (S3 in this case)\n   2. An origin resource that's given permission to alter an S3 bucket\n   3. The resource whose access is being defined\n   4. The user or entity to which access is assigned": "4. In this context, a principal is an entity to which bucket access is assigned.",
    "16. You don't want to open up the contents of an S3 bucket to anyone on the Internet, but you need to share the data with specific clients. Generating and then sending them a presigned URL is a perfect solution. Assuming you didn't explicitly set a value, how long will the presigned URL remain valid?\n   1. 24 hours\n   2. 3,600 seconds\n   3. 5 minutes\n   4. 360 seconds": "2. The default expiry value for a presigned URL is 3,600 seconds (one hour).",
    "17. Which non-S3 AWS resources can improve the security and user experience of your S3-hosted static website? (Choose two.)\n   1. AWS Certificate Manager\n   2. Elastic Compute Cloud (EC2)\n   3. Relational Database Service (RDS)\n   4. Route 53\n   5. AWS Key Management Service": "1, 4. The AWS Certificate Manager (when used as part of a CloudFront distribution) can apply an SSL/TLS encryption certificate to your website. You can use Route 53 to associate a DNS domain name to your site. EC2 instances and RDS database instances would never be used for static websites. You would normally not use KMS for a static website\u2014websites are usually meant to be public and encrypting the website assets with a KMS key would make it impossible for clients to download them.",
    "18. How long will it take to retrieve an archive from Amazon Glacier Deep Archive ?\n   1. 5 hours\n   2. 12 hours\n   3. 2 days\n   4. 1 week": "2. As of this writing, retrieving Glacier Deep Archive data will take no larger than 12 hours.",
    "19. You need a quick way to transfer very large (peta-scale) data archives to the cloud. Assuming your Internet connection isn't up to the task, which of the following will be both (relatively) fast and cost-effective?\n   1. Direct Connect\n   2. Server Migration Service\n   3. Snowball\n   4. Storage Gateway": "3. Direct Connect can provide fast network connections to AWS, but it's very expensive and can take up to 90 days to install. Server Migration Service and Storage Gateway aren't meant for moving data at such scale.",
    "20. Your organization runs Windows-based EC2 instances that all require low-latency read/write access to a single set of files. Which of the following AWS services is your best choice?\n   1. Amazon FSx for Windows File Server\n   2. Amazon FSx for Lustre\n   3. Amazon Elastic File System\n   4. Amazon Elastic Block Store": "1. FSx for Lustre and Elastic File System are primarily designed for access from Linux filesystems. EBS volumes can't be accessed by more than a single instance at a time.",
    "1. What is the range of allowed IPv4 prefix lengths for a VPC CIDR block?\n   1. /16 to /28\n   2. /16 to /56\n   3. /8 to /30\n   4. /56 only": "1. The allowed range of prefix lengths for a VPC CIDR is between /16 and /28 inclusive. The maximum possible prefix length for an IP subnet is /32, so /56 is not a valid length.",
    "2. You've created a VPC with the CIDR 192.168.16.0/24. You want to assign a secondary CIDR to this VPC. Which CIDR can you use?\n   1. 172.31.0.0/16\n   2. 192.168.0.0/16\n   3. 192.168.0.0/24\n   4. 192.168.16.0/23": "3. A secondary CIDR may come from the same RFC 1918 address range as the primary, but it may not overlap with the primary CIDR. 192.168.0.0/24 comes from the same address range (192.168.0.0\u2013192.168.255.255) as the primary and does not overlap with 192.168.16.0/24; 192.168.0.0/16 and 192.168.16.0/23 both overlap with 192.168.16.0/24; and 172.31.0.0/16 is not in the same range as the primary CIDR.",
    "3. You need to create two subnets in a VPC that has a CIDR of 10.0.0.0/16. Which of the following CIDRs can you assign to one of the subnets while leaving room for an additional subnet? (Choose all that apply.)\n   1. 10.0.0.0/24\n   2. 10.0.0.0/8\n   3. 10.0.0.0/16\n   4. 10.0.0.0/23": "1, 4. Options A and D (10.0.0.0/24 and 10.0.0.0/23) are within the VPC CIDR and leave room for a second subnet; 10.0.0.0/8 is wrong because prefix lengths less than /16 aren't allowed; and 10.0.0.0/16 doesn't leave room for another subnet.",
    "4. What is the relationship between a subnet and an availability zone?\n   1. A subnet can exist in multiple availability zones.\n   2. An availability zone can have multiple subnets.\n   3. An availability zone can have only one subnet.\n   4. A subnet's CIDR is derived from its availability zone.": "2. Multiple subnets may exist in a single availability zone. A subnet cannot span availability zones.",
    "5. Which is true regarding an elastic network interface?\n   1. It must have a private IP address from the subnet that it resides in.\n   2. It cannot exist independently of an instance.\n   3. It can be connected to multiple subnets.\n   4. It can have multiple IP addresses from different subnets.": "1. Every ENI must have a primary private IP address. It can have secondary IP addresses, but all addresses must come from the subnet the ENI resides in. Once created, the ENI cannot be moved to a different subnet. An ENI can be created independently of an instance and later attached to an instance.",
    "6. Which of the following statements is true of security groups?\n   1. Only one security group can be attached to an ENI.\n   2. A security group must always be attached to an ENI.\n   3. A security group can be attached to a subnet.\n   4. Every VPC contains a default security group.": "4. Each VPC contains a default security group that can't be deleted. You can create a security group by itself without attaching it to anything. But if you want to use it, you must attach it to an ENI. You also attach multiple security groups to the same ENI.",
    "7. How does an NACL differ from a security group?\n   1. An NACL is stateless.\n   2. An NACL is stateful.\n   3. An NACL is attached to an ENI.\n   4. An NACL can be associated with only one subnet.": "1. An NACL is stateless, meaning it doesn't track a connection state. Every inbound rule must have a corresponding outbound rule to permit traffic, and vice versa. An NACL is attached to a subnet, whereas a security group is attached to an ENI. An NACL can be associated with multiple subnets, but a subnet can have only one NACL.",
    "8. What is an Internet gateway?\n   1. A resource that grants instances in multiple VPCs' Internet access\n   2. An implied router\n   3. A physical router\n   4. A VPC resource with no management IP address": "4. An Internet gateway has no management IP address. It can be associated with only one VPC at a time and so cannot grant Internet access to instances in multiple VPCs. It is a logical VPC resource and not a virtual or physical router.",
    "9. What is the destination for a default IPv4 route?\n   1. 0.0.0.0/0\n   2. ::0/0\n   3. An Internet gateway\n   4. The IP address of the implied router": "1. The destination 0.0.0.0/0 matches all IP prefixes and hence covers all publicly accessible hosts on the Internet. ::0/0 is an IPv6 prefix, not an IPv4 prefix. An Internet gateway is the target of the default route, not the destination.",
    "10. You create a new route table in a VPC but perform no other configuration on it. You then create a new subnet in the same VPC. Which route table will your new subnet be associated with?\n   1. The main route table\n   2. The route table you created\n   3. The default route table\n   4. None of these": "1. Every subnet is associated with the main route table by default. You can explicitly associate a subnet with another route table. There is no such thing as a default route table, but you can create a default route within a route table.",
    "11. You create a Linux instance and have AWS automatically assign a private IP address but not a public IP address. What will happen when you stop and restart the instance?\n   1. You won't be able to establish an SSH session directly to the instance from the Internet.\n   2. The instance won't be able to access the Internet.\n   3. The instance will receive the same private IP address.\n   4. The instance will be unable to reach other instances in its subnet.": "1. An instance must have a public IP address to be directly reachable from the Internet. The instance may be able to reach the Internet via a NAT device. The instance won't necessarily receive the same private IP address because it was automatically assigned. The instance will be able to reach other instances in the subnet because a public IP is not required.",
    "12. How can you assign a public IP address to a running instance that doesn't have one?\n   1. Allocate an ENI and associate it with the instance's primary EIP.\n   2. Allocate an EIP and associate it with the instance's primary ENI.\n   3. Configure the instance to use an automatically assigned public IP.\n   4. Allocate an EIP and change the private IP address of the instance's ENI to match.": "2. Assigning an EIP to an instance is a two-step process. First you must allocate an EIP, and then you must associate it with an ENI. You can't allocate an ENI, and there's no such thing as an instance's primary EIP. Configuring the instance to use an automatically assigned public IP must occur at instance creation. Changing an ENI's private IP to match an EIP doesn't actually assign a public IP to the instance, because the ENI's private address is still private.",
    "13. When an instance with an automatically assigned public IP sends a packet to another instance's EIP, what source address does the destination instance see?\n   1. The public IP\n   2. The EIP\n   3. The private IP\n   4. 0.0.0.0": "1. Internet-bound traffic from an instance with an automatically assigned public IP will traverse an Internet gateway that will perform NAT. The source address will be the instance's public IP. An instance with an automatically assigned public IP cannot also have an EIP. The NAT process will replace the private IP source address with the public IP. Option D, 0.0.0.0, is not a valid source address.",
    "14. Why must a NAT gateway reside in a different subnet than an instance that uses it?\n   1. Both must use different default gateways.\n   2. Both must use different NACLs.\n   3. Both must use different security groups.\n   4. The NAT gateway requires a public interface and a private interface.": "1. The NAT gateway's default route must point to an Internet gateway, and the instance's default route must point to the NAT gateway. No differing NACL configurations between subnets are required to use a NAT gateway. Security groups are applied at the ENI level. A NAT gateway doesn't require separate public and private interfaces.",
    "15. Which of the following is a difference between a NAT instance and a NAT gateway?\n   1. There are different NAT gateway types.\n   2. A NAT instance scales automatically.\n   3. A NAT gateway can span multiple availability zones.\n   4. A NAT gateway scales automatically.": "4. A NAT gateway is a VPC resource that scales automatically to accommodate increased bandwidth requirements. A NAT instance can't do this. A NAT gateway exists in only one availability zone. There are not multiple NAT gateway types. A NAT instance is a regular EC2 instance that comes in different types.",
    "16. Which VPC resource performs network address translation?\n   1. Internet gateway\n   2. Route table\n   3. EIP\n   4. ENI": "1. An Internet gateway performs NAT for instances that have a public IP address. A route table defines how traffic from instances is forwarded. An EIP is a public IP address and can't perform NAT. An ENI is a network interface and doesn't perform NAT.",
    "17. What must you do to configure a NAT instance after creating it?\n   1. Disable the source/destination check on its ENI.\n   2. Enable the source/destination check on its ENI.\n   3. Create a default route in its route table with a NAT gateway as the target.\n   4. Assign a primary private IP address to the instance.": "1. The source/destination check on the NAT instance's ENI must be disabled to allow the instance to receive traffic not destined for its IP and to send traffic using a source address that it doesn't own. The NAT instance's default route must point to an Internet gateway as the target. You can't assign a primary private IP address after the instance is created.",
    "18. Which of the following is true regarding VPC peering?\n   1. Transitive routing is not supported.\n   2. A VPC peering connection requires a public IP address.\n   3. You can peer up to three VPCs using a single peering connection.\n   4. You can use a peering connection to share an Internet gateway among multiple VPCs.": "1. You cannot route through a VPC using transitive routing. Instead, you must directly peer the VPCs containing the instances that need to communicate. A VPC peering connection uses the AWS internal network and requires no public IP address. Because a peering connection is a point-to-point connection, it can connect only two VPCs. A peering connection can be used only for instance-to-instance communication. You can't use it to share other VPC resources.",
    "19. You've created one VPC peering connection between two VPCs. What must you do to use this connection for bidirectional instance-to-instance communication? (Choose all that apply.)\n   1. Create two routes with the peering connection as the target.\n   2. Create only one default route with the peering connection as the target.\n   3. Create another peering connection between the VPCs.\n   4. Configure the instances' security groups correctly.": "1, 4. Each peered VPC needs a route to the CIDR of its peer; therefore, you must create two routes with the peering connection as the target. Creating only one route is not sufficient to enable bidirectional communication. Additionally, the instances' security groups must allow for bidirectional communication. You can't create more than one peering connection between a pair of VPCs.",
    "20. Which of the following can perform stateful traffic filtering? (Choose two.)\n   1. Security groups\n   2. NACLs\n   3. AWS Network Firewall\n   4. AWS Transit Gateway": "1, 3. Security groups and AWS Network Firewall can perform stateful traffic filtering. NACLs perform stateless filtering only. AWS Transit Gateway isn't a firewall and doesn't perform traffic filtering.",
    "21. Which of the following connection types is always encrypted?\n   1. Direct Connect\n   2. VPN\n   3. VPC peering\n   4. Transit gateway": "2. VPN connections are always encrypted.",
    "22. Which of the following allow EC2 instances in different regions to communicate using private IP addresses? (Choose three.)\n   1. VPN\n   2. Direct Connect\n   3. VPC peering\n   4. Transit gateway": "1, 3, 4. VPC peering, transit gateways, and VPNs all allow EC2 instances in different regions to communicate using private IP addresses. Direct Connect is for connecting VPCs to on-premises networks, not for connecting VPCs together.",
    "23. Which of the following is true of a route in a transit gateway route table?\n   1. It can be multicast.\n   2. It can be a blackhole route.\n   3. It can have an Internet gateway as a target.\n   4. It can have an ENI as a target.": "2. A transit gateway route table can hold a blackhole route. If the transit gateway receives traffic that matches the route, it will drop the traffic.",
    "24. Which of the following is an example of a tightly coupled HPC workload?\n   1. Image processing\n   2. Audio processing\n   3. DNA sequencing\n   4. Hurricane track forecasting": "4. Tightly coupled workloads include simulations such as weather forecasting. They can't be broken down into smaller, independent pieces, and so require the entire cluster to function as a single supercomputer.",
    "1. In a relational database, a row may also be called what? (Choose two.)\n   1. Record\n   2. Attribute\n   3. Tuple\n   4. Table": "1, 3. Different relational databases use different terminology. A row, record, and tuple all describe an ordered set of columns. An attribute is another term for column. A table contains rows and columns.",
    "2. What must every relational database table contain?\n   1. A foreign key\n   2. A primary key\n   3. An attribute\n   4. A row": "3. A table must contain at least one attribute or column. Primary and foreign keys are used for relating data in different tables, but they're not required. A row can exist within a table, but a table doesn't need a row in order to exist.",
    "3. Which SQL statement would you use to retrieve data from a relational database table?\n   1. QUERY\n   2. SCAN\n   3. INSERT\n   4. SELECT": "4. The SELECT statement retrieves data from a table. INSERT is used for adding data to a table. QUERY and SCAN are commands used by DynamoDB, which is a nonrelational database.",
    "4. Which relational database type is optimized to handle multiple transactions per second?\n   1. Offline transaction processing (OLTP)\n   2. Online transaction processing (OLTP)\n   3. Online analytic processing (OLAP)\n   4. Key/value store": "2. Online transaction processing databases are designed to handle multiple transactions per second. Online analytics processing databases are for complex queries against large data sets. A key/value store such as DynamoDB can handle multiple transactions per second, but it's not a relational database. There's no such thing as an offline transaction processing database.",
    "5. How many database engines can an RDS database instance run?\n   1. Six\n   2. One\n   3. Two\n   4. Four": "2. Although there are six database engines to choose from, a single database instance can run only one database engine. If you want to run more than one database engine, you will need a separate database instance for each engine.",
    "6. Which database engines are compatible with existing MySQL databases? (Choose all that apply.)\n   1. Microsoft SQL Server\n   2. MariaDB\n   3. Aurora\n   4. PostgreSQL": "2, 3. MariaDB and Aurora are designed as binary drop-in replacements for MySQL. PostgreSQL is designed for compatibility with Oracle databases. Microsoft SQL Server does not support MySQL databases.",
    "7. Which storage engine should you use with MySQL, Aurora, and MariaDB for maximum compatibility with RDS?\n   1. MyISAM\n   2. XtraDB\n   3. InnoDB\n   4. PostgreSQL": "3. InnoDB is the only storage engine Amazon recommends for MySQL and MariaDB deployments in RDS and the only engine Aurora supports. MyISAM is another storage engine that works with MySQL but is not compatible with automated backups. XtraDB is another storage engine for MariaDB, but Amazon no longer recommends it. The PostgreSQL database engine uses its own storage engine by the same name and is not compatible with other database engines.",
    "8. Which database engine supports the bring-your-own-license (BYOL) model? (Choose all that apply.)\n   1. Oracle Standard Edition Two\n   2. Microsoft SQL Server\n   3. Oracle Standard Edition One\n   4. PostgreSQL": "1, 3. All editions of the Oracle database engine support the bring-your-own-license model in RDS. Microsoft SQL Server and PostgreSQL only support the license-included model.",
    "9. Which database instance class provides dedicated bandwidth for storage volumes?\n   1. Standard\n   2. Memory optimized\n   3. Storage optimized\n   4. Burstable performance": "2. Memory-optimized instances are EBS optimized, providing dedicated bandwidth for EBS storage. Standard instances are not EBS optimized and top out at 10,000 Mbps disk throughput. Burstable performance instances are designed for development and test workloads and provide the lowest disk throughput of any instance class. There is no instance class called storage optimized.",
    "10. If a MariaDB database running in RDS needs to write 200 MB of data every second, how many IOPS should you provision using io1 storage to sustain this performance?\n   1. 12,800\n   2. 25,600\n   3. 200\n   4. 16": "1. MariaDB has a page size of 16 KB. To write 200 MB (204,800 KB) of data every second, it would need 12,800 IOPS. Oracle, PostgreSQL, or Microsoft SQL Server, which all use an 8 KB page size, would need 25,600 IOPS to achieve the same throughput. When provisioning IOPS, you must specify IOPS in increments of 1,000, so 200 and 16 IOPS\u2014which would be woefully insufficient anyway\u2014are not valid answers.",
    "11. Using general-purpose SSD storage, how much storage would you need to allocate to get 600 IOPS?\n   1. 200 GB\n   2. 100 GB\n   3. 200 TB\n   4. 200 MB": "1. General-purpose SSD storage allocates three IOPS per gigabyte, up to 10,000 IOPS. Therefore, to get 600 IOPS, you'd need to allocate 200 GB. Allocating 100 GB would give you only 300 IOPS. The maximum storage size for gp2 storage is 16 TB, so 200 TB is not a valid value. The minimum amount of storage you can allocate depends on the database engine, but it's no less than 20 GB, so 200 MB is not valid.",
    "12. If you need to achieve 12,000 IOPS using provisioned IOPS SSD storage, how much storage should you allocate, assuming that you need only 100 GB of storage?\n   1. There is no minimum storage requirement.\n   2. 200 GB.\n   3. 240 GB.\n   4. 12 TB.": "3. When you provision IOPS using io1 storage, you must do so in a ratio no greater than 50 IOPS for 1 GB. Allocating 240 GB of storage would give you 12,000 IOPS. Allocating 200 GB of storage would fall short, yielding just 10,000 IOPS. Allocating 12 TB would be overkill for the amount of storage required.",
    "13. What type of database instance only accepts queries?\n   1. Read replica\n   2. Standby database instance\n   3. Primary database instance\n   4. Master database instance": "1. A read replica only services queries and cannot write to a database. A standby database instance in a multi-AZ deployment does not accept queries. Both a primary and a master database instance can service queries and writes.",
    "14. In a multi-AZ deployment using Oracle, how is data replicated?\n   1. Synchronously from the primary instance to a read replica\n   2. Synchronously using a cluster volume\n   3. Asynchronously from the primary to a standby instance\n   4. Synchronously from the primary to a standby instance": "4. Multi-AZ deployments using Oracle, PostgreSQL, MariaDB, MySQL, or Microsoft SQL Server replicate data synchronously from the primary to a standby instance. Only a multi-AZ deployment using Aurora uses a cluster volume and replicates data to a specific type of read replica called an Aurora replica.",
    "15. Which of the following occurs when you restore a failed database instance from a snapshot?\n   1. RDS restores the snapshot to a new instance.\n   2. RDS restores the snapshot to the failed instance.\n   3. RDS restores only the individual databases to a new instance.\n   4. RDS deletes the snapshot.": "1. When you restore from a snapshot, RDS creates a new instance and doesn't make any changes to the failed instance. A snapshot is a copy of the entire instance, not just a copy of the individual databases. RDS does not delete a snapshot after restoring from it.",
    "16. Which Redshift distribution style stores all tables on all compute nodes?\n   1. EVEN\n   2. ALL\n   3. KEY\n   4. ODD": "2. The ALL distribution style ensures every compute node has a complete copy of every table. The EVEN distribution style splits tables up evenly across all compute nodes. The KEY distribution style distributes data according to the value in a specified column. There is no distribution style called ODD.",
    "17. Which Redshift node type can store up to 326 TB of data?\n   1. Dense memory\n   2. Leader\n   3. Dense storage\n   4. Dense compute": "4. The dense compute type can store up to 326 TB of data on magnetic storage. The dense storage type can store up to 2 PB of data on solid state drives. A leader node coordinates communication among compute nodes but doesn't store any databases. There is no such thing as a dense memory node type.",
    "18. Which is true regarding a primary key in a nonrelational database? (Choose all that apply.)\n   1. It's required to uniquely identify an item.\n   2. It must be unique within the table.\n   3. It's used to correlate data across different tables.\n   4. Its data type can vary within a table.": "1, 2. In a nonrelational database, a primary key is required to uniquely identify an item and hence must be unique within a table. All primary key values within a table must have the same data type. Only relational databases use primary keys to correlate data across different tables.",
    "19. In a DynamoDB table containing orders, which key would be most appropriate for storing an order date?\n   1. Partition key\n   2. Sort key\n   3. Hash key\n   4. Simple primary key": "2. An order date would not be unique within a table, so it would be inappropriate for a partition (hash) key or a simple primary key. It would be appropriate as a sort key, as DynamoDB would order items according to the order date, which would make it possible to query items with a specific date or within a date range.",
    "20. When creating a DynamoDB table, how many read capacity units should you provision to be able to sustain strongly consistent reads of 11 KB per second?\n   1. 3\n   2. 2\n   3. 1\n   4. 0": "1. A single strongly consistent read of an item up to 4 KB consumes one read capacity unit. Hence, reading 11 KB of data per second using strongly consistent reads would consume three read capacity units. Were you to use eventually consistent reads, you would need only two read capacity units, as one eventually consistent read gives you up to 8 KB of data per second. Regardless, you must specify a read capacity of at least 1, so 0 is not a valid answer.",
    "21. Which Redshift node type can provide the fastest read access?\n   1. Dense compute\n   2. Dense storage\n   3. Leader\n   4. KEY": "2. The dense storage node type uses fast SSDs, whereas the dense compute node uses slower magnetic storage. The leader node doesn't access the database but coordinates communication among compute nodes. KEY is a data distribution strategy Redshift uses, but there is no such thing as a key node.",
    "22. Which DynamoDB index type allows the partition and hash key to differ from the base table?\n   1. Eventually consistent index\n   2. Local secondary index\n   3. Global primary index\n   4. Global secondary index": "4. When you create a table, you can choose to create a global secondary index with a different partition and hash key. A local secondary index can be created after the table is created, but the partition key must be the same as the base table, although the hash key can be different. There is no such thing as a global primary index or eventually consistent index.",
    "23. To ensure the best performance, in which of the following situations would you choose to store data in a NoSQL database instead of a relational database?\n   1. You need to perform a variety of complex queries against the data.\n   2. You need to query data based on only one attribute.\n   3. You need to store JSON documents.\n   4. The data will be used by different applications.": "2. NoSQL databases are optimized for queries against a primary key. If you need to query data based only on one attribute, you'd make that attribute the primary key. NoSQL databases are not designed for complex queries. Both NoSQL and relational databases can store JSON documents, and both database types can be used by different applications.",
    "24. What type of database can discover how different items are related to each other?\n   1. SQL\n   2. Relational\n   3. Document-oriented store\n   4. Graph\n   5. Video processing": "4. A graph database is a type of nonrelational database that discovers relationships among items. A document-oriented store is a nonrelational database that analyzes and extracts data from documents. Relational databases can enforce relationships between records but don't discover them. A SQL database is a type of relational database.",
    "1. Which of the following is the greatest risk posed by using your AWS account root user for day-to-day operations?\n   1. There would be no easy way to control resource usage by project or class.\n   2. There would be no effective limits on the effect of an action, making it more likely for unintended and unwanted consequences to result.\n   3. Since root has full permissions over your account resources, an account compromise at the hands of hackers would be catastrophic.\n   4. It would make it difficult to track which account user is responsible for specific actions.": "3. Although each of the other options represents possible concerns, none of them carries consequences as disastrous as the complete loss of control over your account.",
    "2. You're trying to create a custom IAM policy to more closely manage access to components in your application stack. Which of the following syntax-related statements is a correct description of IAM policies?\n   1. The Action element refers to the way IAM will react to a request.\n   2. The * character applies an element globally\u2014as broadly as possible.\n   3. The Resource element refers to the third-party identities that will be allowed to access the account.\n   4. The Effect element refers to the anticipated resource state after a request is granted.": "2. The * character does, indeed, represent global application. The Action element refers to the kind of action requested (list, create, etc.), the Resource element refers to the particular AWS account resource that's the target of the policy, and the Effect element refers to the way IAM should react to a request.",
    "3. Which of the following will\u2014when executed on its own\u2014prevent an IAM user with no existing policies from launching an EC2 instance? (Choose three.)\n   1. Attach no policies to the user.\n   2. Attach two policies to the user, with one policy permitting full EC2 access and the other permitting IAM password changes but denying EC2 access.\n   3. Attach a single policy permitting the user to create S3 buckets.\n   4. Attach the AdministratorAccess policy.\n   5. Associate an IAM action statement blocking all EC2 access to the user's account.": "1, 2, 3. Unless there's a policy that explicitly allows an action, it will be denied. Therefore, a user with no policies or with a policy permitting S3 actions doesn't permit EC2 instance permissions. Similarly, when two policies conflict, the more restrictive will be honored. The AdministratorAccess policy opens up nearly all AWS resources, including EC2. There's no such thing as an IAM action statement.",
    "4. Which of the following are important steps for securing IAM user accounts? (Choose two.)\n   1. Never use the account to perform any administration operations.\n   2. Enable multifactor authentication (MFA).\n   3. Assign a long and complex password.\n   4. Delete all access keys.\n   5. Insist that your users access AWS resources exclusively through the AWS CLI.": "2, 3. If you don't perform any administration operations with regular IAM users, then there really is no point for them to exist. Similarly, without access keys, there's a limit to what a user will be able to accomplish. Ideally, all users should use MFA and strong passwords. The AWS CLI is an important tool, but it isn't necessarily the most secure.",
    "5. To reduce your exposure to possible attacks, you're auditing the active access keys associated with your account. Which of the following AWS CLI commands can tell you whether a specified access key is still being used?\n   1. aws iam get-access-key-used \u2013access-key-id <key_ID>\n   2. aws iam --get-access-key-last-used access-key-id <key_ID>\n   3. aws iam get-access-key-last-used access-last-key-id <key_ID>\n   4. aws iam get-access-key-last-used \u2013-access-key-id <key_ID>": "4. The top-level command is iam, and the correct subcommand isget-access-key-last-used. The parameter is identified by --access-last-key-id. Parameters (not subcommands) are always prefixed with -- characters.",
    "6. You're looking to reduce the complexity and tedium of AWS account administration. Which of the following is the greatest benefit of organizing your users into groups?\n   1. It enhances security by consolidating resources.\n   2. It simplifies the management of user permissions.\n   3. It allows for quicker response times to service interruptions.\n   4. It simplifies locking down the root user.": "2. IAM groups are primarily about simplifying administration. They have no direct impact on resource usage or response times and only an indirect impact on locking down the root user.",
    "7. During an audit of your authentication processes, you enumerate a number of identity types and want to know which of them might fit the category of \u201ctrusted identity\u201d and require deeper investigation. Which of these is not considered a trusted entity in the context of IAM roles?\n   1. A web identity authenticating with Google\n   2. An identity coming through a SAML-based federated provider\n   3. An identity using an X.509 certificate\n   4. A web identity authenticating with Amazon Cognito": "3. X.509 certificates are used for encrypting SOAP requests, not authentication. The other choices are all valid identities within the context of an IAM role.",
    "8. Your company is bidding for a contract with a U.S. government agency that demands any cryptography modules used on the project be compliant with government standards. Which of the following AWS services provides virtual hardware devices for managing encryption infrastructure that's FIPS 140-2 compliant?\n   1. AWS CloudHSM\n   2. AWS Key Management Service\n   3. AWS Security Token Service\n   4. AWS Secrets Manager": "1. AWS CloudHSM provides encryption that's FIPS 140-2 compliant. Key Management Service manages encryption infrastructure but isn't FIPS 140-2 compliant. Security Token Service is used to issue tokens for valid IAM roles, and Secrets Manager handles secrets for third-party services or databases.",
    "9. Which of the following is the best tool for authenticating access to a VPC-based Microsoft SharePoint farm?\n   1. Amazon Cognito\n   2. AWS Directory Service for Microsoft Active Directory\n   3. AWS Secrets Manager\n   4. AWS Key Management Service": "2. AWS Directory Service for Microsoft Active Directory provides Active Directory authentication within a VPC environment. Amazon Cognito provides user administration for your applications. AWS Secrets Manager handles secrets for third-party services or databases. AWS Key Management Service manages encryption infrastructure.",
    "10. What is the function of Amazon Cognito identity pools?\n   1. Gives your application users temporary, controlled access to other services in your AWS account\n   2. Adds user sign-up and sign-in to your applications\n   3. Incorporates encryption infrastructure into your application life cycle\n   4. Delivers up-to-date credentials to authenticate RDS database requests": "1. Identity pools provide temporary access to defined AWS services to your application users. Sign-up and sign-in is managed through Cognito user pools. KMS and/or CloudHSM provide encryption infrastructure. Credential delivery to databases or third-party applications is provided by AWS Secrets Manager.",
    "11. An employee with access to the root user on your AWS account has just left your company. Since you can't be 100 percent sure that the former employee won't try to harm your company, which of the following steps should you take? (Choose three.)\n   1. Change the password and MFA settings for the root account.\n   2. Delete and re-create all existing IAM policies.\n   3. Change the passwords for all your IAM users.\n   4. Delete the former employee's own IAM user (within the company account).\n   5. Immediately rotate all account access keys.": "1, 4, 5. Options A, D, and E are appropriate steps. Your IAM policies will be as effective as ever, even if outsiders know your policies. Since even an account's root user would never have known other users' passwords, there's no reason to change them.",
    "12. You need to create a custom IAM policy to give one of your developers limited access to your DynamoDB resources. Which of the following elements will not play any role in crafting an IAM policy?\n   1. Action\n   2. Region\n   3. Effect\n   4. Resource": "2. IAM policies are global\u2014they're not restricted to any one region. Policies do, however, require an action (like create buckets), an effect (allow), and a resource (S3).",
    "13. Which of the following are necessary steps for creating an IAM role? (Choose two.)\n   1. Define the action.\n   2. Select at least one policy.\n   3. Define a trusted entity.\n   4. Define the consumer application.": "2, 3. IAM roles require a defined trusted entity and at least one policy. However, the relevant actions are defined by the policies you choose, and roles themselves are uninterested in which applications use them.",
    "14. Which of the following uses authentication based on AWS Security Token Service (STS) tokens?\n   1. Policies\n   2. Users\n   3. Groups\n   4. Roles": "4. STS tokens are used as temporary credentials to external identities for resource access to IAM roles. Users and groups would not use tokens to authenticate, and policies are used to define the access a token will provide, not the recipient of the access.",
    "15. What format must be used to write an IAM policy?\n   1. HTML\n   2. Key/value pairs\n   3. JSON\n   4. XML": "3. Policies must be written in JSON format.",
    "16. If you need to allow a user full control over EC2 instance resources, which two of the following must be included in the policy you create? (Choose two.)\n   1. \"Target\": \"ec2:*\"\n   2. \"Action\": \"ec2:*\"\n   3. \"Resource\": \"ec2:*\"\n   4. \"Effect\": \"Allow\"\n   5. \"Effect\": \"Permit\"": "2, 4. The correct Resource line would read \"Resource\": \"*\". And the correct Action line would read \"Action\": \"*\". There is no \"Target\" line in an IAM policy. \"Permit\" is not a valid value for \"Effect\".",
    "17. What is the function of Amazon Cognito user pools?\n   1. Gives your application users temporary, controlled access to other services in your AWS account\n   2. Adds user sign-up and sign-in to your applications\n   3. Incorporates encryption infrastructure into your application life cycle\n   4. Delivers up-to-date credentials to authenticate RDS database requests": "2. User pools provide sign-up and sign-in for your application's users. Temporary access to defined AWS services to your application users is provided by identity pools. KMS and/or CloudHSM provide encryption infrastructure. Credential delivery to databases or third-party applications is provided by AWS Secrets Manager.",
    "18. Which of the following best describe the \u201cmanaged\u201d part of AWS Managed Microsoft AD? (Choose two.)\n   1. Integration with on-premises AD domains is possible.\n   2. AD domain controllers are launched in two availability zones.\n   3. Data is automatically replicated.\n   4. Underlying AD software is automatically updated.": "3, 4. An AWS managed service takes care of all underlying infrastructure management for you. In this case, that will include data replication and software updates. On-premises integration and multi-AZ deployment are important infrastructure features, but they're not unique to \u201cmanaged\u201d services.",
    "19. Which of the following steps are part of the access key rotation process? (Choose three.)\n   1. Monitor the use of your new keys.\n   2. Monitor the use of old keys.\n   3. Deactivate the old keys.\n   4. Delete the old keys.\n   5. Confirm the status of your X.509 certificate.": "2, 3, 4. Options B, C, and D are all parts of the key rotation process. In this context, key usage monitoring is only useful to ensure that none of your applications is still using an old key that's set to be retired. X.509 certificates aren't used for access keys.",
    "20. What tool will allow an Elastic Container Service task to access container images it might need that are being maintained in your account's Elastic Container Registry?\n   1. An IAM role\n   2. An IAM policy\n   3. An IAM group\n   4. An AIM access key": "1. You attach IAM roles to services in order to give them permissions over resources in other services within your account.",
    "1. You've configured CloudTrail to log all management events in all regions. Which of the following API events will CloudTrail log? (Choose all that apply.)\n   1. Logging into the AWS Console\n   2. Creating an S3 bucket from the web console\n   3. Uploading an object to an S3 bucket\n   4. Creating a subnet using the AWS CLI": "2, 4. Creating a bucket and subnet are API actions, regardless of whether they're performed from the web console or AWS CLI. Uploading an object to an S3 bucket is a data event, not a management event. Logging into the AWS Console is a non-API management event.",
    "2. You've configured CloudTrail to log all read-only data events. Which of the following events will CloudTrail log?\n   1. Viewing all S3 buckets\n   2. Uploading a file to an S3 bucket\n   3. Downloading a file from an S3 bucket\n   4. Creating a Lambda function": "3. Data events include S3 object-level activity and Lambda function executions. Downloading an object from S3 is a read-only event. Uploading a file to an S3 bucket is a write-only event and hence would not be logged by the trail. Viewing an S3 bucket and creating a Lambda function are management events, not data events.",
    "3. Sixty days ago, you created a trail in CloudTrail to log read-only management events. Subsequently someone deleted the trail. Where can you look to find out who deleted it? No other trails are configured.\n   1. The IAM user log\n   2. The trail logs stored in S3\n   3. The CloudTrail event history in the region where the trail was configured\n   4. The CloudTrail event history in any region": "3. CloudTrail stores 90 days of event history for each region, regardless of whether a trail is configured. Event history is specific to the events occurring in that region. Because the trail was configured to log read-only management events, the trail logs would not contain a record of the trail's deletion. They might contain a record of who viewed the trail, but that would be insufficient to establish who deleted it. There is no such thing as an IAM user log.",
    "4. What uniquely distinguishes two CloudWatch metrics that have the same name and are in the same namespace?\n   1. The region\n   2. The dimension\n   3. The timestamp\n   4. The data point": "2. CloudWatch uses dimensions to uniquely identify metrics with the same name and namespace. Metrics in the same namespace will necessarily be in the same region. The data point of a metric and the timestamp that it contains are not unique and can't be used to uniquely identify a metric.",
    "5. Which type of monitoring sends metrics to CloudWatch every five minutes?\n   1. Regular\n   2. Detailed\n   3. Basic\n   4. High resolution": "3. Basic monitoring sends metrics every five minutes, whereas detailed monitoring sends them every minute. CloudWatch can store metrics at regular or high resolution, but this affects how the metric is timestamped, rather than the frequency with which it's delivered to CloudWatch.",
    "6. You update a custom CloudWatch metric with the timestamp of 15:57:08 and a value of 3. You then update the same metric with the timestamp of 15:57:37 and a value of 6. Assuming the metric is a high-resolution metric, which of the following will CloudWatch do?\n   1. Record both values with the given timestamp.\n   2. Record the second value with the timestamp 15:57:37, overwriting the first value.\n   3. Record only the first value with the timestamp 15:57:08, ignoring the second value.\n   4. Record only the second value with the timestamp 15:57:00, overwriting the first value.": "1. CloudWatch can store high-resolution metrics at subminute resolution. Therefore, updating a metric at 15:57:08 and again at 15:57:37 will result in CloudWatch storing two separate data points. Only if the metric were regular resolution would CloudWatch overwrite an earlier data point with a later one. Under no circumstances would CloudWatch ignore a metric update.",
    "7. How long does CloudWatch retain metrics stored at one-hour resolution?\n   1. 15 days\n   2. 3 hours\n   3. 63 days\n   4. 15 months": "4. Metrics stored at one-hour resolution age out after 15 months. Five-minute resolutions are stored for 63 days. One-minute resolution metrics are stored for 15 days. High-resolution metrics are kept for 3 hours.",
    "8. You want to use CloudWatch to graph the exact data points of a metric for the last hour. The metric is stored at five-minute resolution. Which statistic and period should you use?\n   1. The Sum statistic with a five-minute period\n   2. The Average statistic with a one-hour period\n   3. The Sum statistic with a one-hour period\n   4. The Sample count statistic with a five-minute period": "1. To graph a metric's data points, specify the Sum statistic and set the period equal to the metric's resolution, which in this case is five minutes. Graphing the Sum or Average statistic over a one-hour period will not graph the metric's data points but rather the Sum or Average of those data points over a one-hour period. Using the Sample count statistic over a five-minute period will yield a value of 1 for each period, since there's only one data point per period.",
    "9. Which CloudWatch resource type stores log events?\n   1. Log group\n   2. Log stream\n   3. Metric filter\n   4. CloudWatch Agent": "2. CloudWatch uses a log stream to store log events from a single source. Log groups store and organize log streams but do not directly store log events. A metric filter extracts metrics from logs but doesn't store anything. The CloudWatch agent can deliver logs to CloudWatch from a server but doesn't store logs.",
    "10. The CloudWatch Agent on an instance has been sending application logs to a CloudWatch log stream for several months. How can you remove old log events without disrupting delivery of new log events? (Choose all that apply.)\n   1. Delete the log stream.\n   2. Manually delete old log events.\n   3. Set the retention of the log stream to 30 days.\n   4. Set the retention of the log group to 30 days.": "1, 4. Every log stream must be in a log group. The retention period setting of a log group controls how long CloudWatch retains log events within those streams. You can't manually delete log events individually, but you can delete all events in a log stream by deleting the stream. You can't set a retention period on a log stream directly.",
    "11. You created a trail to log all management events in all regions and send the trail logs to CloudWatch logs. You notice that some recent management events are missing from the log stream, but others are there. What are some possible reasons for this? (Choose all that apply.)\n   1. The missing events are greater than 256 KB in size.\n   2. The metric filter is misconfigured.\n   3. There's a delay between the time the event occurs and the time CloudTrail streams the event to CloudWatch.\n   4. The IAM role that CloudTrail assumes is misconfigured.": "1, 3. CloudTrail will not stream events greater than 256 KB in size. There's also a normal delay, typically up to 15 minutes, before an event appears in a CloudWatch log stream. Metric filters have no bearing on what log events get put into a log stream. Although a misconfigured or missing IAM role would prevent CloudTrail from streaming logs to CloudWatch, the question indicates that some events are present. Hence, the IAM role is correctly configured.",
    "12. Two days ago, you created a CloudWatch alarm to monitor the VolumeReadOps on an EBS volume. Since then, the alarm has remained in an INSUFFICIENT_DATA state. What are some possible reasons for this? (Choose all that apply.)\n   1. The data points to monitor haven't crossed the specified threshold.\n   2. The EBS volume isn't attached to a running instance.\n   3. The evaluation period hasn't elapsed.\n   4. The alarm hasn't collected enough data points to alarm.": "2, 4. If an EBS volume isn't attached to a running instance, EBS won't generate any metrics to send to CloudWatch. Hence, the alarm won't be able to collect enough data points to alarm. The evaluation period can be no more than 24 hours, and the alarm was created two days ago, so the evaluation period has elapsed. The data points to monitor don't have to cross the threshold for CloudWatch to determine the alarm state.",
    "13. You want a CloudWatch alarm to change state when four consecutive evaluation periods elapse with no data. How should you configure the alarm to treat missing data?\n   1. As Missing\n   2. Breaching\n   3. Not Breaching\n   4. Ignore\n   5. As Not Missing": "2. To have CloudWatch treat missing data as exceeding the threshold, set the Treat Missing Data As option to Breaching. Setting it to Not Breaching will have the opposite effect. Setting it to As Missing will cause CloudWatch to ignore the missing data and behave as if those evaluation periods didn't occur. The Ignore option causes the alarm not to change state in response to missing data. There's no option to treat missing data as Not Missing.",
    "14. You've configured an alarm to monitor a metric in the AWS/EC2 namespace. You want CloudWatch to send you a text message and reboot an instance when an alarm is breaching. Which two actions should you configure in the alarm? (Choose two.)\n   1. SMS action\n   2. Auto Scaling action\n   3. Notification action\n   4. EC2 action": "3, 4. CloudWatch can use the Simple Notification Service to send a text message. CloudWatch refers to this as a Notification action. To reboot an instance, you must use an EC2 action. The Auto Scaling action will not reboot an instance. SMS is not a valid CloudWatch alarm action.",
    "15. In a CloudWatch alarm, what does the EC2 recover action do to the monitored instance?\n   1. Migrates the instance to a different host\n   2. Reboots the instance\n   3. Deletes the instance and creates a new one\n   4. Restores the instance from a snapshot": "1. The recover action is useful when there's a problem with an instance that requires AWS involvement to repair, such as a hardware failure. The recover action migrates the same instance to a new host. Rebooting an instance assumes the instance is running and entails the instance remaining on the same host. Recovering an instance does not involve restoring any data from a snapshot, as the instance retains the same EBS volume(s).",
    "16. You learn that an instance in the us-west-1 region was deleted at some point in the past. To find out who deleted the instance and when, which of the following must be true?\n   1. The AWS Config configuration recorder must have been turned on in the region at the time the instance was deleted.\n   2. CloudTrail must have been logging write-only management events for all regions.\n   3. CloudTrail must have been logging IAM events.\n   4. The CloudWatch log stream containing the deletion event must not have been deleted.": "2. If CloudTrail were logging write-only management events in the same region as the instance, it would have generated trail logs containing the deletion event. Deleting a log stream containing CloudTrail events does not delete those events from the trail logs stored in S3. Deleting an EC2 instance is not an IAM event. If AWS Config were tracking changes to EC2 instances in the region, it would have recorded a timestamped configuration item for the deletion, but it would not include the principal that deleted the instance.",
    "17. Which of the following may be included in an AWS Config delivery channel? (Choose all that apply.)\n   1. A CloudWatch log stream\n   2. The delivery frequency of the configuration snapshot\n   3. An S3 bucket name\n   4. An SNS topic ARN": "2, 3, 4. The delivery channel must include an S3 bucket name and may specify an SNS topic and the delivery frequency of configuration snapshots. You can't specify a CloudWatch log stream.",
    "18. You configured AWS Config to monitor all your resources in the us-east-1 region. After making several changes to the AWS resources in this region, you decided you want to delete the old configuration items. How can you accomplish this?\n   1. Pause the configuration recorder.\n   2. Delete the configuration recorder.\n   3. Delete the configuration snapshots.\n   4. Set the retention period to 30 days and wait for the configuration items to age out.": "4. You can't delete configuration items manually, but you can have AWS Config delete them after no less than 30 days. Pausing or deleting the configuration recorder will stop AWS Config from recording new changes but will not delete configuration items. Deleting configuration snapshots, which are objects stored in S3, will not delete the configuration items.",
    "19. Which of the following metric math expressions can CloudWatch graph? (Choose all that apply.)\n   1. AVG(m1)-m1\n   2. AVG(m1)\n   3. METRICS()/AVG(m1)\n   4. m1/m2": "3, 4. CloudWatch can graph only a time series. METRICS()/AVG(m1) and m1/m2 both return a time series. AVG(m1)-m1 and AVG(m1) return scalar values and can't be graphed directly.",
    "20. You've configured an AWS Config rule to check whether CloudTrail is enabled. What could prevent AWS Config from evaluating this rule?\n   1. Turning off the configuration recorder\n   2. Deleting the rule\n   3. Deleting the configuration history for CloudTrail\n   4. Failing to specify a frequency for periodic checks": "2. Deleting the rule will prevent AWS Config from evaluating resources configurations against it. Turning off the configuration recorder won't prevent AWS Config from evaluating the rule. It's not possible to delete the configuration history for a resource from AWS Config. When you specify a frequency for periodic checks, you must specify a valid frequency, or AWS Config will not accept the configuration.",
    "21. Which of the following would you use to execute a Lambda function whenever an EC2 instance is launched?\n   1. CloudWatch Alarms\n   2. EventBridge\n   3. CloudTrail\n   4. CloudWatch Metrics": "2. EventBridge can take an action in response to an event, such as an EC2 instance launch. CloudWatch Alarms can take an action based only on a metric. CloudTrail logs events but doesn't generate any alerts by itself. CloudWatch Metrics is used for graphing metrics.",
    "1. Which of the following describes the function of a name server?\n   1. Translating human-readable domain names into IP addresses\n   2. Registering domain names with ICANN\n   3. Registering domain names with VeriSign\n   4. Applying routing policies to network packets": "1. Option A is the correct answer. Name servers resolve IP addresses from domain names, allowing clients to connect to resources. Domain registration is performed by domain name registrars. Routing policies are applied through record sets within hosted zones.",
    "2. Your organization is planning a new website and you're putting together all the pieces of information you'll need to complete the project. Which of the following describes a domain?\n   1. An object's FQDN\n   2. Policies controlling the way remote requests are resolved\n   3. One or more servers, data repositories, or other digital resources identified by a single domain name\n   4. A label used to direct network requests to a domain's resources": "3. A domain is a set of resources identified by a single domain name. FQDN stands for fully qualified domain name. Policies for resolving requests are called routing policies.",
    "3. You need to decide which kind of website name will best represent its purpose. Part of that task will involve choosing a top-level domain (TLD). Which of the following is an example of a TLD?\n   1. amazon.com/documentation/\n   2. aws.\n   3. amazon.\n   4. .com": "4. The rightmost section of an FQDN address is the TLD. aws. would be a subdomain or host, amazon. is the SLD, and amazon.com/documentation/ points to a resource stored at the web root of the domain server.",
    "4. Which of the following is the name of a record type\u2014 as used\u2014in a zone file?\n   1. CNAME (canonical name)\n   2. TTL (time to live)\n   3. Record type\n   4. Record data": "1. CNAME is a record type. TTL, record type, and record data are all configuration elements, not record types.",
    "5. Which of the following DNS record types should you use to associate a domain name with an IP address?\n   1. NS\n   2. SOA\n   3. A\n   4. CNAME": "3. An A record maps a hostname to an IPv4 address. NS records identify name servers. SOA records document start of authority data. CNAME records define one hostname as an alias for another.",
    "6. Which of the following are services provided by Amazon Route 53? (Choose three.)\n   1. Domain registration\n   2. Content delivery network\n   3. Health checks\n   4. DNS management\n   5. Secure and fast direct network connections to an AWS VPC": "1, 3, 4. Route 53 provides domain registration, health checks, and DNS management. Content delivery network services are provided by CloudFront. Secure and fast network connections to a VPC can be created using AWS Direct Connect.",
    "7. For regulatory compliance, your application may only provide data to requests coming from the United States. Which of the following routing policies can be configured to do this?\n   1. Simple\n   2. Latency\n   3. Geolocation\n   4. Multivalue": "3. Geolocation can control routing by the geographic origin of the request. The simple policy sends traffic to a single resource. Latency sends content using the fastest origin resource. Multivalue can be used to make a deployment more highly available.",
    "8. Your web application is hosted within multiple AWS regions. Which of the following routing policies will ensure the fastest possible access for your users?\n   1. Latency\n   2. Weighted\n   3. Geolocation\n   4. Failover": "1. Latency selects the available resource with the lowest latency. Weighted policies route among multiple resources by percentage. Geolocation tailors request responses to the end user's location but isn't concerned with response speed. Failover incorporates backup resources for higher availability.",
    "9. You're testing three versions of a new application, with each version running on its own server and the current production version on a fourth server. You want to route 15 percent of your total traffic to each of the test servers and route the remaining 85 percent of traffic to the production server. Which routing policy will you use?\n   1. Failover\n   2. Weighted\n   3. Latency\n   4. Geolocation": "2. Weighted policies route among multiple resources by percentage. Failover incorporates backup resources for higher availability. Latency selects the available resource with the lowest latency. Geolocation tailors request responses to the end user's location.",
    "10. You have production infrastructure in one region sitting behind one DNS domain, and for disaster recovery purposes, you have parallel infrastructure on standby in a second AWS region behind a second domain. Which routing policy will automate the switchover in the event of a failure in the production system?\n   1. Latency\n   2. Weighted\n   3. Geolocation\n   4. Failover": "4. Failover incorporates backup resources for higher availability. Latency selects the available resource with the lowest latency. Weighted policies route among multiple resources by percentage. Geolocation tailors request responses to the end user's location.",
    "11. Which of the following kinds of hosted zones are real options within Route 53? (Choose two.)\n   1. Public\n   2. Regional\n   3. VPC\n   4. Private\n   5. Hybrid": "1, 4. Public and private hosting zones are real options. Regional, hybrid, and VPC zones don't exist (although private zones do map to VPCs).",
    "12. Which of the following actions will you need to perform to transfer a domain from an external registrar to Route 53? (Choose two.)\n   1. Unlock the domain transfer setting on the external registrar admin page.\n   2. Request an authorization code from the external registrar.\n   3. Copy the name server addresses from Route 53 to the external registrar admin page.\n   4. Create a hosted zone CNAME record set.": "1, 2. To transfer a domain, you'll need to make sure the domain isn't set to locked. You'll also need an authorization code that you'll provide to Route 53. Copying name server addresses is necessary only for managing domains that are hosted on but not registered with Route 53. CNAME record sets are used to define one hostname as an alias for another.",
    "13. Which of the following actions will you need to perform to use Route 53 to manage a domain that's being hosted on an external registrar?\n   1. Request an authorization code from the external registrar.\n   2. Copy the name server addresses from Route 53 to the external registrar admin page.\n   3. Create a hosted zone CNAME record set.\n   4. Unlock the domain transfer setting on the external registrar admin page.": "2. You can enable remotely registered domains on Route 53 by copying name server addresses into the remote registrar-provided interface (not the other way around). Making sure the domain isn't set to locked and requesting authorization codes are used to transfer a domain to Route 53, not just to manage the routing. CNAME record sets are used to define one hostname as an alias for another.",
    "14. Your multiserver application has been generating quality-related complaints from users and your logs show some servers are underused and others have been experiencing intermittent failures. How do Route 53 health checks test for the health of a resource so that a failover policy can direct your users appropriately?\n   1. It periodically tries to load the index.php page.\n   2. It periodically tries to load the index.html page.\n   3. It periodically tries to load a specified web page.\n   4. It periodically tries to log into the resource using SSH.": "3. You specify the web page that you want used for testing when you configure your health check. There is no default page. Remote SSH sessions would be impossible for a number of reasons and wouldn't definitively confirm a running resource in any case.",
    "15. Which of the following most accurately describes the difference between geolocation and geoproximity routing policies?\n   1. Geoproximity policies specify geographic areas by their relationship either to a particular longitude and latitude or to an AWS region, whereas geolocation policies use the continent, country, or U.S. state where the request originated to decide what resource to send.\n   2. Geolocation policies specify geographic areas by their relationship either to a particular longitude and latitude or to an AWS region, whereas geoproximity policies use the continent, country, or U.S. state where the request originated to decide what resource to send.\n   3. Geolocation policies will direct traffic to the resource you identify as primary as long as health checks confirm that that resource is running properly, whereas geoproximity policies allow you to deliver web pages in customer-appropriate languages.\n   4. Geolocation policies use a health check configuration routing to make a deployment more highly available, whereas geoproximity policies leverage resources running in multiple AWS regions to provide service to clients from the instances that will deliver the best experience.": "1. Geoproximity is about precisely pinpointing users, whereas geolocation uses geopolitical boundaries.",
    "16. Which of the following are challenges that CloudFront is well positioned to address? (Choose two.)\n   1. A heavily used website providing media downloads for a global audience\n   2. An S3 bucket with large media files used by workers on your corporate campus\n   3. A file server accessed through a corporate VPN\n   4. A popular website with periodically changing content": "1, 4. CloudFront is optimized for handling heavy download traffic and for caching website content. Users on a single corporate campus or accessing resources through a VPN will not benefit from the distributed delivery provided by CloudFront.",
    "17. Which of the following is not a permitted origin for a CloudFront distribution?\n   1. Amazon S3 bucket\n   2. AWS MediaPackage channel endpoint\n   3. API Gateway endpoint\n   4. Web server": "3. API Gateway is used to generate custom client SDKs for your APIs to connect your backend systems to mobile, web, and server applications or services.",
    "18. What's the best way to control the costs your CloudFront distribution incurs?\n   1. Select a price class that maintains copies in only a limited subset of CloudFront's edge locations.\n   2. Configure a custom SSL certificate to restrict access to HTTPS requests only.\n   3. Disable the use of Alternate Domain Names (CNAMES) for your distribution.\n   4. Enable Compress Objects Automatically for your distribution.": "1. Choosing a price class offering limited distribution is the best way to reduce costs. Non-HTTPS traffic can be excluded (thereby saving some money) but not through the configuration of an SSL certificate (you'd need further configuration). Disabling Alternate Domain Names or enabling Compress Objects Automatically won't reduce costs.",
    "19. Which of the following is not necessarily a direct benefit of using a CloudFront distribution?\n   1. User requests from an edge location that's recently received the same request will be delivered with lower latency.\n   2. CloudFront distributions can be directly mapped to Route 53 hosted zones.\n   3. All user requests will be delivered with lower latency.\n   4. You can incorporate free encryption certificates into your infrastructure.": "3. Not every CloudFront distribution is optimized for low-latency service. Requests of an edge location will only achieve lower latency after copies of your origin files are already cached. Therefore, a response to the first request might not be fast because CloudFront still has to copy the file from the origin server.",
    "20. Which of the following content types is the best fit for a Real-Time Messaging Protocol (RTMP) distribution?\n   1. Amazon Elastic Transcoder\u2013based videos\n   2. S3-based videos\n   3. Streaming videos\n   4. A mix of text and media-rich digital content": "2. RTMP distributions can manage content only from S3 buckets. RTMP is intended for the distribution of video content.",
    "1. What are valid use cases for transforming data when importing it into a data lake? (Select three.)\n   1. Imposing consistent timestamps\n   2. Removing corrupted data\n   3. Creating a schema\n   4. Removing duplicate data\n   5. Visualizing data": "1, 2, 4. Data transformation includes changing the contents or format of data. Creating a schema is something you do with structured data, and a data lake doesn't impose or require a schema. Visualizing data is something that occurs with data that's already stored in the data lake.",
    "2. What AWS Data Lake transform detects duplicate data?\n   1. MatchFinder\n   2. FindMatches ML\n   3. Elastic MapReduce\n   4. Spark": "2. FindMatches ML is the name of the transform that detects duplicate data. MatchFinder doesn't exist. Spark is a big data framework. Elastic MapReduce allows searching and sorting large data sets.",
    "3. What's the most efficient way to import data from an on-premises SQL database into an AWS Data Lake?\n   1. Dump the database into an S3 bucket and then import the data into the data lake.\n   2. Import the data into RDS and then into the data lake.\n   3. Use the Glue Connector.\n   4. Use the JDBC connector.": "4. Connecting to the SQL database using the JDBC connector and importing the data directly into the data lake is the most efficient solution. The Glue Connector isn't real. The other options are feasible but inefficient.",
    "4. What protocols does AWS Transfer Family support? (Choose two.)\n   1. SFTP\n   2. SMB\n   3. FTP\n   4. CIFS\n   5. HTTPS": "1, 3. AWS Transfer Family supports SFTP, FTPS, and FTP. It doesn't support CIFS/SMB or HTTPS for file transfer.",
    "5. AWS Transfer Family can be used to transfer files to or from which of the following? (Choose two.)\n   1. EBS\n   2. EFS\n   3. RDP\n   4. S3\n   5. DynamoDB": "2, 4. AWS Transfer Family can transfer files to or from EFS or S3. It doesn't support transfers with EBS or DynamoDB. RDP is not a file storage system.",
    "6. What technology does AWS Glue use to search large data sets and perform data transformation?\n   1. Amazon Athena\n   2. Apache Spark\n   3. Apache Elephant Stack\n   4. AWS Data Lake": "2. AWS Glue uses the Apache Spark big data framework to perform search and transforms.",
    "7. What is the difference between a data lake and a data warehouse? (Choose two.)\n   1. A data warehouse can store unstructured data.\n   2. A data warehouse is a relational database.\n   3. A data lake requires structured data.\n   4. A data lake can store unstructured, schema-less data.": "2, 4. A data warehouse is a relational OLAP database. A data lake can store both structured and unstructured, nonrelational data.",
    "8. Which of the following can AWS Data Lake import from? (Choose two.)\n   1. EBS\n   2. ELB\n   3. CloudFront\n   4. IAM\n   5. CloudWatch": "2, 3. AWS Data Lake can import data from ELB and CloudFront, but none of the others.",
    "9. Which of the following can analyze data in an AWS Data Lake? (Choose two.)\n   1. Amazon EMS\n   2. Athena\n   3. RedShift Spectrum\n   4. RedShift\n   5. S3": "2, 3. Athena and RedShift Spectrum can analyze data in an AWS Data Lake. RedShift and S3 cannot. Amazon EMS doesn't exist.",
    "10. Which of the following is not an appropriate use of AWS Glue?\n   1. Searching data\n   2. Ingesting real-time streaming data\n   3. Preparing data for analysis\n   4. Transforming data": "2. Kinesis is more appropriate for real-time streaming data. For all other cases, AWS Glue is the best.",
    "11. You're developing an application to predict future weather patterns based on RADAR images. Which of the following Kinesis services is the best choice to support this application?\n   1. Kinesis Data Streams\n   2. Kinesis Video Streams\n   3. Kinesis Data Firehose\n   4. Kinesis ML": "2. Kinesis Video Streams is designed to work with time-indexed data such as RADAR images. Kinesis ML doesn't exist.",
    "12. You're streaming image data to Kinesis Data Streams and need to retain the data for 30 days. How can you do this? (Choose two.)\n   1. Create a Kinesis Data Firehose delivery stream.\n   2. Increase the stream retention period to 14 days.\n   3. Specify an S3 bucket as the destination.\n   4. Specify CloudWatch Logs as the destination.": "1, 3. You can't specify a retention period over 7 days, so your only option is to create a Kinesis Data Firehose delivery stream that receives data from the Kinesis Data Stream and sends the data to an S3 bucket.",
    "13. Which of the following Kinesis services requires you to specify a destination for the stream?\n   1. Kinesis Video Streams\n   2. Kinesis Data Streams\n   3. Kinesis Data Firehose\n   4. Kinesis Data Warehouse": "3. Kinesis Data Firehose requires you to specify a destination for a delivery stream. Kinesis Video Streams and Kinesis Data Streams use a producer-consumer model that allows consumers to subscribe to a stream. There is no such thing as Kinesis Data Warehouse.",
    "14. You're running an on-premises application that frequently writes to a log file. You want to stream this log file to a Kinesis Data Stream. How can you accomplish this with the least effort?\n   1. Use the CloudWatch Logs Agent.\n   2. Use the Amazon Kinesis Agent.\n   3. Write a script that uses the Kinesis Producer Library.\n   4. Move the application to an EC2 instance.": "2. The Amazon Kinesis Agent can automatically stream the contents of a file to Kinesis. There's no need to write any custom code or move the application to EC2. The CloudWatch Logs Agent can't send logs to a Kinesis Data Stream.",
    "15. When deciding whether to use SQS or Kinesis Data Streams to ingest data, which of the following should you take into account?\n   1. The frequency of data\n   2. The total amount of data\n   3. The number of consumers that need to receive the data\n   4. The order of data": "3. SQS and Kinesis Data Streams are similar. But SQS is designed to temporarily hold a small message until a single consumer processes it, whereas Kinesis Data Streams is designed to provide durable storage and playback of large data streams to multiple consumers.",
    "16. You want to send streaming log data into Amazon Redshift. Which of the following services should you use? (Choose two.)\n   1. SQS with a standard queue\n   2. Kinesis Data Streams\n   3. Kinesis Data Firehose\n   4. SQS with a FIFO queue": "2, 3. You should stream the log data to Kinesis Data Streams and then have Kinesis Data Firehose consume the data and stream it to Redshift.",
    "17. Which of the following is not an appropriate use case for Kinesis?\n   1. Stock feeds\n   2. Facial recognition\n   3. Static website hosting\n   4. Videoconferencing": "3. Kinesis is for streaming data such as stock feeds and video. Static websites are not streaming data.",
    "18. You need to push 2 MB per second through a Kinesis Data Stream. How many shards do you need to configure?\n   1. 1\n   2. 2\n   3. 4\n   4. 8": "2. Shards determine the capacity of a Kinesis Data Stream. A single shard gives you writes of up to 1 MB per second, so you'd need two shards to get 2 MB of throughput.",
    "19. Multiple consumers are receiving a Kinesis Data Stream at a total rate of 3 MB per second. You plan to add more consumers and need the stream to support reads of at least 5 MB per second. How many shards do you need to add?\n   1. 1\n   2. 2\n   3. 3\n   4. 4": "1. Shards determine the capacity of a Kinesis Data Stream. Each shard supports 2 MB of reads per second. Because consumers are already receiving a total of 3 MB per second, it implies you have at least two shards already configured, supporting a total of 4 MB per second. Therefore, to support 5 MB per second you need to add just one more shard.",
    "20. Which of the following does Kinesis Data Firehose not support?\n   1. Videoconferencing\n   2. Transforming video metadata\n   3. Converting CSV to JSON\n   4. Redshift": "1. Kinesis Data Firehose is designed to funnel streaming data to big data applications, such as Redshift or Hadoop. It's not designed for videoconferencing.",
    "1. What's the minimum level of availability you need to stay under 30 minutes of downtime per month?\n   1. 99 percent\n   2. 99.9 percent\n   3. 99.95 percent\n   4. 99.999 percent": "3. Availability of 99.95 percent translates to about 22 minutes of downtime per month, or 4 hours and 23 minutes per year. Availability of 99.999 percent is less than 30 seconds of downtime per month, but the question calls for the minimum level of availability. Availability of 99 percent yields more than 7 hours of downtime per month, whereas 99.9 percent is more than 43 minutes of downtime per month.",
    "2. Your application runs on two EC2 instances in one availability zone. An elastic load balancer distributes user traffic evenly across the healthy instances. The application on each instance connects to a single RDS database instance. Assuming each EC2 instance has an availability of 90 percent and the RDS instance has an availability of 95 percent, what is the total application availability?\n   1. 94.05 percent\n   2. 99 percent\n   3. 99.9 percent\n   4. 99.95 percent": "1. The EC2 instances are redundant components, so to calculate their availability, you multiply the component failure rates and subtract the product from 100 percent. In this case, 100% \u2013 (10% \u00d7 10%) = 99%. Because the database represents a hard dependency, you multiply the availability of the EC2 instances by the availability of the RDS instance, which is 95 percent. In this case, 99% \u00d7 95% = 94.05%. A total availability of 99 percent may seem intuitive, but because the redundant EC2 instances have a hard dependency on the RDS instance, you must multiply the availabilities together. A total availability of 99.99 percent is unachievable since it's well above the availability of any of the components.",
    "3. Your organization is designing a new application to run on AWS. The developers have asked you to recommend a database that will perform well in all regions. Which database should you recommend for maximum availability?\n   1. Multi-AZ RDS using MySQL\n   2. DynamoDB\n   3. Multi-AZ RDS using Aurora\n   4. A self-hosted SQL database": "2. DynamoDB offers 99.99 percent availability and low latency. Because it's distributed, data is stored across multiple availability zones. You can also use DynamoDB global tables to achieve even higher availability: 99.999 percent. Multi-AZ RDS offerings can provide low-latency performance, particularly when using Aurora, but the guaranteed availability is capped at 99.95 percent. Hosting your own SQL database isn't a good option because, although you could theoretically achieve high availability, it would come at the cost of significant time and effort.",
    "4. Which of the following can help you increase the availability of a web application? (Choose all that apply.)\n   1. Store web assets in an S3 bucket instead of on the application instance.\n   2. Use instance classes large enough to handle your application's peak load.\n   3. Scale your instances in.\n   4. Scale your instances out.": "2, 4. One cause of application failures is resource exhaustion. By scoping out large enough instances and scaling out to make sure you have enough of them, you can prevent failure and thus increase availability. Scaling instances in may help with cost savings but won't help availability. Storing web assets in S3 instead of hosting them from an instance can help with performance but won't have an impact on availability.",
    "5. You've configured an EC2 Auto Scaling group to use a launch configuration to provision and install an application on several instances. You now need to reconfigure Auto Scaling to install an additional application on new instances. Which of the following should you do?\n   1. Modify the launch configuration.\n   2. Create a launch template and configure the Auto Scaling group to use it.\n   3. Modify the launch template.\n   4. Modify the CloudFormation template.": "2. You can modify a launch template by creating a new version of it; however, the question indicates that the Auto Scaling group was created using a launch configuration. You can't modify a launch configuration. Auto Scaling doesn't use CloudFormation templates.",
    "6. You create an Auto Scaling group with a minimum group size of 3, a maximum group size of 10, and a desired capacity of 5. You then manually terminate two instances in the group. Which of the following will Auto Scaling do?\n   1. Create two new instances.\n   2. Reduce the desired capacity to 3.\n   3. Nothing.\n   4. Increment the minimum group size to 5.": "1. Auto Scaling strives to maintain the number of instances specified in the desired capacity setting. If the desired capacity setting isn't set, Auto Scaling will attempt to maintain the number of instances specified by the minimum group size. Given a desired capacity of 5, there should be five healthy instances. If you manually terminate two of them, Auto Scaling will create two new ones to replace them. Auto Scaling will not adjust the desired capacity or minimum group size.",
    "7. Which of the following can Auto Scaling use for instance health checks? (Choose all that apply.)\n   1. ELB health checks\n   2. CloudWatch Alarms\n   3. Route 53 health checks\n   4. EC2 system checks\n   5. EC2 instance checks": "1, 4, 5. Auto Scaling monitors the health of instances in the group using either ELB or EC2 instance and system checks. It can't use Route 53 health checks. Dynamic scaling policies can use CloudWatch Alarms, but these are unrelated to checking the health of instances.",
    "8. You're running an application that receives a spike in traffic on the first day of every month. You want to configure Auto Scaling to add more instances before the spike begins and then add additional instances in proportion to the CPU utilization of each instance. Which of the following should you implement? (Choose all that apply.)\n   1. Target tracking policies\n   2. Scheduled actions\n   3. Step scaling policies\n   4. Simple scaling policies": "2, 3. Scheduled actions can adjust the minimum and maximum group sizes and the desired capacity on a schedule, which is useful when your application has a predictable load pattern. To add more instances in proportion to the aggregate CPU utilization of the group, implement step scaling policies. Target tracking policies adjust the desired capacity of a group to keep the threshold of a given metric near a predefined value. Simple scaling policies simply add more instances when a defined CloudWatch alarm triggers, but the number of instances added is not proportional to the value of the metric.",
    "9. Which of the following provide the most protection against data corruption and accidental deletion for existing objects stored in S3? (Choose two.)\n   1. Versioning\n   2. Bucket policies\n   3. Cross-region replication\n   4. Using the Standard storage class": "1, 4. Enabling versioning protects objects against data corruption and deletion by keeping before and after copies of every object. The Standard storage class replicates objects across multiple availability zones in a region, guarding against the failure of an entire zone. Bucket policies may protect against accidental deletion, but they don't guard against data corruption. Cross-region replication applies to new objects, not existing ones.",
    "10. You need to maintain three days of backups for binary files stored across several EC2 instances in a spot fleet. What's the best way to achieve this?\n   1. Stream the files to CloudWatch Logs.\n   2. Create an Elastic File System and back up the files to it using a cron job.\n   3. Create a Snapshot Lifecycle Policy to snapshot each instance every 24 hours and retain the latest three snapshots.\n   4. Create a Snapshot Lifecycle Policy to snapshot each instance every 4 hours and retain the latest 18 snapshots.": "3. The Data Lifecycle Manager can automatically create snapshots of an EBS volume every 12 or 24 hours and retain up to 1,000 snapshots. Backing up files to EFS is not an option because a spot instance may terminate before the cron job has a chance to complete. CloudWatch Logs doesn't support storing binary files.",
    "11. You plan to run multi-AZ RDS across three availability zones in a region. You want to have two read replicas per zone. Which database engine should you choose?\n   1. MySQL\n   2. PostgreSQL\n   3. MySQL\n   4. Aurora": "4. Aurora allows you to have up to 15 replicas. MariaDB, MySQL, and PostgreSQL allow you to have only up to five.",
    "12. You're running an RDS instance in one availability zone. What should you implement to be able to achieve a recovery point objective (RPO) of five minutes?\n   1. Configure multi-AZ.\n   2. Enable automated snapshots.\n   3. Add a read replica in the same region.\n   4. Add a read replica in a different region.": "2. When you enable automated snapshots, RDS backs up database transaction logs about every five minutes. Configuring multi-AZ will enable synchronous replication between the two instances, but this is useful for avoiding failures and is unrelated to the time it takes to recover a database. Read replicas are not appropriate for disaster recovery because data is copied to them asynchronously, and there can be a significant delay in replication, resulting in an RPO of well over five minutes.",
    "13. When creating subnets in a VPC, what are two reasons to leave sufficient space in the VPC for more subnets later? (Choose two.)\n   1. You may need to add another tier for your application.\n   2. You may need to implement RDS.\n   3. AWS occasionally adds more availability zones to a region.\n   4. You may need to add a secondary CIDR to the VPC.": "1, 3. AWS sometimes adds additional availability zones to a region. To take advantage of a new zone, you'll need to be able to add a new subnet in it. You also may decide later that you may need another subnet or tier for segmentation or security purposes. RDS doesn't require a separate subnet. It can share the same subnet with other VPC resources. Adding a secondary CIDR to a VPC doesn't require adding another subnet.",
    "14. You plan to deploy 50 EC2 instances, each with two private IP addresses. To put all of these instances in a single subnet, which subnet CIDRs could you use? (Choose all that apply.)\n   1. 172.21.0.0/25\n   2. 172.21.0.0/26\n   3. 10.0.0.0/8\n   4. 10.0.0.0/21": "1, 4. Fifty EC2 instances, each with two private IP addresses, would consume 100 IP addresses in a subnet. Additionally, AWS reserves five IP addresses in every subnet. The subnet therefore must be large enough to hold 105 IP addresses. 172.21.0.0/25 and 10.0.0.0/21 are sufficiently large. 172.21.0.0/26 allows room for only 63 IP addresses. 10.0.0.0/8 is large enough, but a subnet prefix length must be at least /16.",
    "15. You're currently connecting to your AWS resources using a 10 Gbps Internet connection at your office. You also have end users around the world who access the same AWS resources. What are two reasons you may consider using Direct Connect in addition to your Internet connection? (Choose two.)\n   1. Lower latency\n   2. Higher bandwidth\n   3. Better end-user experience\n   4. Increased security": "1, 4. Direct Connect offers consistent speeds and latency to the AWS cloud. Because Direct Connect bypasses the public Internet, it's more secure. For speeds, you can choose 1 Gbps or 10 Gbps, so Direct Connect wouldn't offer a bandwidth increase over using the existing 10 Gbps Internet connection. Adding a Direct Connect connection wouldn't have an effect on end-user experience, since they would still use the Internet to reach your AWS resources.",
    "16. Before connecting a VPC to your datacenter, what must you do to ensure proper connectivity?\n   1. Use IAM policies to restrict access to AWS resources.\n   2. Ensure the IP address ranges in the networks don't overlap.\n   3. Ensure security groups on your datacenter firewalls are properly configured.\n   4. Use in-transit encryption.": "2. When connecting a VPC to an external network, whether via a VPN connection or Direct Connect, make sure the IP address ranges don't overlap. In-transit encryption, though useful for securing network traffic, isn't required for proper connectivity. IAM policies restrict API access to AWS resources, but this is unrelated to network connectivity. Security groups are VPC constructs and aren't something you configure on a datacenter firewall.",
    "17. You plan to run a stand-alone Linux application on AWS and need 99 percent availability. The application doesn't require a database, and only a few users will access it. You will occasionally need to terminate and re-create the instance using a different AMI. Which of the following should you use? (Choose all that apply.)\n   1. CloudFormation\n   2. Auto Scaling\n   3. User data\n   4. Dynamic scaling policies": "1, 3. CloudFormation lets you provision and configure EC2 instances by defining your infrastructure as code. This lets you update the AMI easily and build a new instance from it as needed. You can include application installation scripts in the user data to automate the build process. Auto Scaling isn't appropriate for this scenario because you're going to sometimes terminate and re-create the instance. Dynamic scaling policies are part of Auto Scaling.",
    "18. You need eight instances running simultaneously in a single region. Assuming three availability zones are available, what's the minimum number of instances you must run in each zone to be able to withstand a single zone failure?\n   1. 3\n   2. 16\n   3. 8\n   4. 4": "4. By running four instances in each zone, you have a total of 12 instances in the region. If one zone fails, you lose four of those instances and are left with eight. Running eight or 16 instances in each zone would allow you to withstand one zone failure, but the question asks for the minimum number of instances. Three instances per zone would give you nine total in the region, but if one zone fails, you'd be left with only six.",
    "19. If your application is down for 45 minutes a year, what is its approximate availability?\n   1. 99 percent\n   2. 99.9 percent\n   3. 99.99 percent\n   4. 99.95 percent": "3. Availability of 99.99 percent corresponds to about 52 minutes of downtime per year; 99 percent, 99.9 percent, and 99.95 percent entail significantly more downtime.",
    "20. You're running an application in two regions and using multi-AZ RDS with read replicas in both regions. Users normally access the application in only one region by browsing to a public domain name that resolves to an elastic load balancer. If that region fails, which of the following should you do to fail over to the other region? (Choose all that apply.)\n   1. Update the DNS record to point to the load balancer in the other region.\n   2. Point the load balancer to the other region.\n   3. Fail over to the database in the other region.\n   4. Restore the database from a snapshot.": "1, 3. Because users access a public domain name that resolves to an elastic load balancer, you'll need to update the DNS record to point to the load balancer in the other region. You'll also need to fail the database over to the other region so that the read replica can become the primary. Load balancers are not cross-region, so it's not possible to point the load balancer in one region to instances in another. Restoring the database isn't necessary because the primary database instance asynchronously replicates data to the read replicas in the other region.",
    "21. When a consumer grabs a message from an SQS queue, what happens to the message? (Choose two.)\n   1. It is immediately deleted from the queue.\n   2. It remains in the queue for 30 seconds and is then deleted.\n   3. It stays in the queue for the remaining duration of the retention period.\n   4. It becomes invisible to other consumers for the duration of the visibility timeout.": "3, 4. After a consumer grabs a message, the message is not deleted. Instead, the message becomes invisible to other consumers for the duration of the visibility timeout. The message is automatically deleted from the queue after it's been in there for the duration of the retention period.",
    "22. What is the default visibility timeout for an SQS queue?\n   1. 0 seconds\n   2. 30 seconds\n   3. 12 hours\n   4. 7 days": "2. The default visibility timeout for a queue is 30 seconds. It can be configured to between 0 seconds and 12 hours.",
    "23. What is the default retention period for an SQS queue?\n   1. 30 minutes\n   2. 1 hour\n   3. 1 day\n   4. 4 days\n   5. 7 days\n   6. 14 days": "4. The default retention period is 4 days but can be set to between 1 minute and 14 days.",
    "24. You want to make sure that only specific messages placed in an SQS queue are not available for consumption for 10 minutes. Which of the following settings can you use to achieve this?\n   1. Delay queue\n   2. Message timer\n   3. Visibility timeout\n   4. Long polling": "2. You can use a message timer to hide a message for up to 15 minutes. Per-queue delay settings apply to all messages in the queue unless you specifically override the setting using a message timer.",
    "25. Which of the following SQS queue types can handle over 50,000 in-flight messages?\n   1. FIFO\n   2. Standard\n   3. Delay\n   4. Short": "2. A standard queue can handle up to 120,000 in-flight messages. A FIFO queue can handle up to about 20,000. Delay and short are not valid queue types.",
    "26. What SQS queue type always delivers messages in the order they were received?\n   1. FIFO\n   2. Standard\n   3. LIFO\n   4. FILO\n   5. Basic": "1. FIFO queues always deliver messages in the order they were received. Standard queues usually do as well, but they're not guaranteed to. LIFO, FILO, and basic aren't valid queue types.",
    "27. You have an application that uses long polling to retrieve messages from an SQS queue. Occasionally, the application crashes because of duplicate messages. Which of the following might resolve the issue?\n   1. Configure a per-queue delay.\n   2. Use a standard queue.\n   3. Use a FIFO queue.\n   4. Use short polling.": "3. Standard queues may occasionally deliver a message more than once. FIFO queues will not. Using long polling alone doesn't result in duplicate messages.",
    "28. A producer application places messages in an SQS queue, and consumer applications poll the queue every 5 seconds using the default polling method. Occasionally, when a consumer polls the queue, SQS reports there are no messages in the queue, even though there are. When the consumer subsequently polls the queue, SQS delivers the messages. Which of the following may explain the missing messages?\n   1. Using long polling\n   2. Using short polling\n   3. Using a FIFO queue\n   4. Using a standard queue": "2. Short polling, which is the default, may occasionally fail to deliver messages. To ensure delivery of these messages, use long polling.",
    "29. Which of the following situations calls for a dead-letter queue?\n   1. A message sits in the queue for too long and gets deleted.\n   2. Different consumers receive and process the same message.\n   3. Messages are mysteriously disappearing from the queue.\n   4. A consumer repeatedly fails to process a particular message.": "4. Dead-letter queues are for messages that a consumer is unable to process. To use a dead-letter queue, you create a queue of the same type as the source queue and set the maxReceiveCount to the maximum number of times a message can be received before it's moved to the dead-letter queue.",
    "30. A message that's 6 days old is sent to a dead-letter queue. The retention period for the dead-letter queue and the source queue is 10 days. What will happen to the message?\n   1. It will sit in the dead-letter queue for up to 10 days.\n   2. It will be immediately deleted.\n   3. It will be deleted after four days.\n   4. It will sit in the dead-letter queue for up to 20 days.": "3. If the retention period for the dead-letter queue is 10 days, and a message is already 6 days old when it's moved to the dead-letter queue, it will spend at most 4 days in the dead-letter queue before being deleted.",
    "1. Which of the following are parameters used to describe the performance of specific EC2 instance types? (Choose three.)\n   1. ECUs (EC2 compute units)\n   2. vCPUs (virtual CPUs)\n   3. ACCpR (Aggregate Cumulative Cost per Request)\n   4. Intel AES-NI\n   5. Maximum read replicas": "1, 2, 4. ECUs, vCPUs, and the Intel AES-NI encryption set are all instance type parameters. Aggregate cumulative cost per request has nothing to do with EC2 instances but is a common key performance indicator (KPI). Read replicas are a feature used with database engines.",
    "2. As the popularity of your EC2-based application grows, you need to improve your infrastructure so that it can better handle fluctuations in demand. Which of the following are normally necessary components for successful Auto Scaling? (Choose three.)\n   1. Launch configuration\n   2. Load balancer\n   3. Custom-defined EC2 AMI\n   4. A start.sh script\n   5. An AWS OpsWorks stack": "1, 2, 3. A launch configuration pointing to an EC2 AMI and an associated load balancer are all, normally, essential to an Auto Scaling operation. Passing a startup script to the instance at runtime may not be necessary, especially if your application is already set up as part of your AMI. OpsWorks stacks are orchestration automation tools and aren't necessary for successful Auto Scaling.",
    "3. Which of the following best describes the role that launch configurations play in Auto Scaling?\n   1. Define the capacity metric that will trigger a scaling change.\n   2. Define the AMI to be deployed by Auto Scaling operations.\n   3. Control the minimum and maximum number of instances to allow.\n   4. Define the associated load balancer.": "2. Defining a capacity metric, minimum and maximum instances, and a load balancer are all done during Auto Scaling configuration. Only the AMI is defined by the launch configuration.",
    "4. You're considering building your new e-commerce application using a microservices architecture where individual servers are tasked with separate but complementary tasks (document server, database, cache, etc.). Which of the following is probably the best platform?\n   1. Elastic Container Service\n   2. Lambda\n   3. ECR\n   4. Elastic Beanstalk": "1. Elastic Container Service is a good platform for microservices. Lambda functions executions are short-lived (having a 15-minute maximum) and wouldn't work well for this kind of deployment. Beanstalk operations aren't ideal for microservices. ECR is a repository for container images and isn't a deployment platform on its own.",
    "5. Your EC2 deployment profile would benefit from a traditional RAID configuration for the EBS volumes you're using. Where are RAID-optimized EBS volume configurations performed?\n   1. From the EBS dashboard\n   2. From the EC2 Storage Optimization dashboard\n   3. From the AWS CLI\n   4. From within the EC2 instance OS": "4. RAID optimization is an OS-level configuration and can, therefore, be performed only from within the OS.",
    "6. Which of the following tools will provide both low-latency access and resilience for your S3-based data?\n   1. CloudFront\n   2. RAID arrays\n   3. Cross-region replication\n   4. Transfer Acceleration": "3. Cross-region replication can provide both low-latency and resilience. CloudFront and S3 Transfer Acceleration deliver low latency but not resilience. RAID arrays can deliver both, but only on EBS volumes.",
    "7. Which of the following tools uses CloudFront edge locations to speed up data transfers?\n   1. Amazon S3 Transfer Acceleration\n   2. S3 Cross-Region Replication\n   3. EBS Data Transfer Wizard\n   4. EC2 Auto Scaling": "1. S3 Transfer Acceleration makes use of CloudFront locations. Neither S3 Cross-Region Replication nor EC2 Auto Scaling uses CloudFront edge locations, and the EBS Data Transfer Wizard doesn't exist (although perhaps it should).",
    "8. Your multi-tiered application has been experiencing slower than normal data reads and writes. As you work on improving performance, which of the following is not a major design consideration for a managed RDS database?\n   1. Optimizing indexes\n   2. Optimizing scalability\n   3. Optimizing schemas\n   4. Optimizing views": "2. Scalability is managed automatically by RDS, and there is no way for you to improve it through user configurations. Indexes, schemas, and views should be optimized as much as possible.",
    "9. Which of the following are possible advantages of hosting a relational database on an EC2 instance over using the RDS service? (Choose two.)\n   1. Automated software patches\n   2. Automated OS updates\n   3. Out-of-the-box Auto Scaling\n   4. Cost savings\n   5. Greater host control": "4, 5. Automated patches, out-of-the-box Auto Scaling, and updates are benefits of a managed service like RDS, not of custom-built EC2-based databases.",
    "10. You've received complaints from users that performance on your EC2-based graphics processing application is slower than normal. Demand has been rising over the past month or two, which could be a factor. Which of the following is the most likely to help? (Choose two.)\n   1. Moving your application to Amazon LightSail\n   2. Switching to an EC2 instance that supports enhanced graphics\n   3. Deploying Amazon Elasticsearch in front of your instance\n   4. Increasing the instance limit on your Auto Scaling group\n   5. Putting your application behind a CloudFront distribution": "2, 4. Integrated enhanced graphics and Auto Scaling can both help here. Amazon LightSail is meant for providing quick and easy compute deployments. Elasticsearch isn't likely to help with a graphics workload. CloudFront can help with media transfers, but not with graphics processing.",
    "11. Which of the following load balancer types is optimized for TCP-based applications and preserves the source IP address?\n   1. Application load balancer\n   2. Classic load balancer\n   3. Network load balancer\n   4. Gateway load balancer": "3. The network load balancer is designed for any TCP-based application and preserves the source IP address. The application load balancer terminates HTTP and HTTPS connections, and it's designed for applications running in a VPC, but it doesn't preserve the source IP address. The Classic load balancer works with any TCP-based application but doesn't preserve the source IP address. A gateway load balancer is meant for virtual appliance workloads.",
    "12. Which of the following can be used to configure a CloudFormation template? (Choose three.)\n   1. The CloudFormation drag-and-drop interface\n   2. Selecting a prebuilt sample template\n   3. Importing a template from AWS CloudDeploy\n   4. Creating your own JSON template document\n   5. Importing a template from Systems Manager": "1, 2, 4. The CloudFormation wizard, prebuilt templates, and JSON formatting are all useful for CloudFormation deployments. CloudDeploy and Systems Manager are not good sources for CloudFormation templates.",
    "13. Which of the following details is not a necessary component of a CloudFormation configuration?\n   1. Default node name\n   2. Stack name\n   3. Database name\n   4. DB User name": "1. There is no default node name in a CloudFormation configuration\u2014nor is there a node of any sort.",
    "14. Which of the following can be integrated into your AWS workflow through AWS OpsWorks? (Choose two.)\n   1. Ansible\n   2. Chef\n   3. Terraform\n   4. SaltStack\n   5. Puppet": "2, 5. Chef and Puppet are both integrated with AWS OpsWorks. Terraform, SaltStack, and Ansible are not directly integrated with OpsWorks.",
    "15. Which of the following are important elements of a successful resource monitoring protocol? (Choose two.)\n   1. CloudWatch dashboards\n   2. CloudWatch OneView\n   3. SNS alerts\n   4. AWS Config dashboards": "1, 3. Dashboards and SNS are important elements of resource monitoring. There are no tools named CloudWatch OneView or AWS Config dashboards.",
    "16. Which of the following will most enhance the value of the CloudWatch data your resources generate? (Choose two.)\n   1. Predefined performance baselines\n   2. Predefined key performance indicators (KPIs)\n   3. Advance permission from AWS\n   4. A complete record of your account's resource configuration changes\n   5. A running Service Catalog task": "1, 2. Advance permission from AWS is helpful only for penetration testing operations. A complete record of your account's resource configuration changes would make sense in the context of AWS Config, but not CloudWatch. Service Catalog helps you audit your resources but doesn't contribute to ongoing event monitoring.",
    "17. Which of the following can be used to audit the changes made to your account and resource configurations?\n   1. AWS CloudTrail\n   2. AWS CloudWatch\n   3. AWS CodePipeline\n   4. AWS Config": "4. Config is an auditing tool. CloudTrail tracks API calls. CloudWatch monitors system performance. CodePipeline is a continuous integration/continuous deployment (CI/CD) orchestration service.",
    "18. Which of the following caching engines can be integrated with Amazon ElastiCache? (Choose two.)\n   1. Varnish\n   2. Redis\n   3. Memcached\n   4. Nginx": "2, 3. ElastiCache executions can use either Redis or Memcached. Varnish and Nginx are both caching engines but are not integrated into ElastiCache.",
    "19. Which of the following use case scenarios are a good fit for caching using Redis and ElastiCache? (Choose two.)\n   1. Your online application requires users' session states to be saved and the behavior of all active users to be compared.\n   2. Your online application needs the fastest operation available.\n   3. Your admin is not familiar with caching and is looking for a relatively simple setup for a straightforward application performance improvement.\n   4. You're not sure what your application needs might be in a month or two, so you want to leave open as many options as possible.": "1, 4. Redis is useful for operations that require persistent session states and or greater flexibility. If you're after speed, Redis might not be the best choice; in many cases, Memcached will provide faster service. Redis configuration has a rather steep learning curve.",
    "20. Which of the following database engines is not a candidate for read replicas within Amazon RDS?\n   1. MySQL\n   2. DynamoDB\n   3. MariaDB\n   4. PostgreSQL": "2. DynamoDB is not available at all in RDS, and certainly not as a read replica.",
    "21. When using CloudFormation to provision multiple stacks of related resources, by which of the following should you organize your resources into different stacks? (Choose two.)\n   1. Cost\n   2. S3 bucket\n   3. Life cycle\n   4. Ownership": "3, 4. It's a best practice to organize stacks by life cycle (e.g., development, test, production) and ownership (e.g., network team, development team). You can store templates for multiple stacks in the same bucket, and there's no need to separate templates for different stacks into different buckets. Organizing stacks by resource cost doesn't offer any advantage since the cost is the same regardless of which stack a resource is in.",
    "22. Which of the following resource properties are good candidates for definition as parameters in a CloudFormation template? (Choose two.)\n   1. AMI ID\n   2. EC2 key pair name\n   3. Stack name\n   4. Logical ID": "1, 2. Parameters let you input custom values into a template when you create a stack. The purpose of parameters is to avoid hard-coding those values into a template. An AMI ID and EC2 key pair name are values that likely would not be hard-coded into a template. Although you define the stack name when you create a stack, it is not a parameter that you define in a template. The logical ID of a resource must be hard-coded in the template.",
    "23. You want to use nested stacks to create an EC2 Auto Scaling group and the supporting VPC infrastructure. These stacks do not need to pass any information to stacks outside of the nested stack hierarchy. Which of the following must you add to the template that creates the Auto Scaling group?\n   1. An Export field to the Output section\n   2. A resource of the type AWS::EC2::VPC\n   3. A resource of the type AWS::CloudFormation::Stack\n   4. Fn::ImportValue": "3. When using nested stacks, the parent stack defines a resource of the type AWS::CloudFormation::Stack, which points to the template used to generate the nested stack. Because of this, there's no need to define a VPC resource directly in the template that creates the parent stack. There is also no need to export stack output values because the nested stacks do not need to pass any information to stacks outside of the nested stack hierarchy. For this same reason, you don't need to use the Fn::ImportValue intrinsic function, since it is used to import values exported by another stack.",
    "24. You need to update a stack that has a stack policy applied. What must you do to verify the specific resources CloudFormation will change before updating the stack?\n   1. Create a change set.\n   2. Perform a direct update.\n   3. Update the stack policy.\n   4. Override the stack policy.": "1. A change set lets you see the changes CloudFormation will make before updating the stack. A direct update doesn't show you the changes before making them. There's no need to update or override the stack policy before using a change set to view the changes that CloudFormation would make.",
    "1. Which of the following options can you not set in a password policy? (Choose two.)\n   1. Maximum length\n   2. Require the use of numbers\n   3. Prevent multiple users from using the same password\n   4. Require an administrator to reset an expired password": "1, 3. A password policy can specify a minimum password length but not a maximum. It can prevent a user from reusing a password they used before but not one that another user has used. A password policy can require a password to contain numbers. It can also require administrator approval to reset an expired password.",
    "2. An IAM user is attached to a customer-managed policy granting them sufficient access to carry out their duties. You want to require multifactor authentication (MFA) for this user to use the AWS CLI. What element should you change in the policy?\n   1. Resource\n   2. Condition\n   3. Action\n   4. Principal": "2. The Condition element lets you require MFA to grant the permissions defined in the policy. The Resource and Action elements define what those permissions are but not the conditions under which those permissions are granted. The Principal element is not used in an identity-based policy.",
    "3. You created an IAM policy that another administrator subsequently modified. You need to restore the policy to its original state but don't remember how it was configured. What should you do to restore the policy? (Choose two.)\n   1. Consult CloudTrail global management event logs.\n   2. Restore the policy from a snapshot.\n   3. Consult CloudTrail data event logs.\n   4. Revert to the previous policy version.": "1, 4. IAM keeps five versions of every customer-managed policy. When CloudTrail is configured to log global management events, it will record any policy changes in the request parameters of the CreatePolicyVersion operation. There is no such thing as a policy snapshot. CloudTrail data event logs will not log IAM events.",
    "4. An IAM user with full access to all EC2 actions in all regions assumes a role that has access to only the EC2 RunInstances operation in the us-east-1 region. What will the user be able to do under the assumed role?\n   1. Create a new instance in any region.\n   2. Create a new instance in the us-east-1 region.\n   3. Start an existing instance in the us-east-1 region.\n   4. Start an existing instance in any region.": "2. When an IAM user assumes a role, the user gains the permissions assigned to that role but loses the permissions assigned to the IAM user. The RunInstances action launches a new instance. Because the role can perform the RunInstances action in the us-east-1 region, the user, upon assuming the role, can create a new instance in the us-east-1 region but cannot perform any other actions. StartInstances starts an existing instance but doesn't launch a new one.",
    "5. Several objects in a S3 bucket are encrypted using a KMS customer master key. Which of the following will give an IAM user permission to decrypt these objects?\n   1. Add the user to the key policy as a key user.\n   2. Grant the user access to the key using an IAM policy.\n   3. Add the user to the key policy as a key administrator.\n   4. Add the user as a principal to the bucket policy.": "1. Granting a user access to use a KMS key to decrypt data requires adding the user to the key policy as a key user. Adding the user as a key administrator is insufficient to grant this access, as is granting the user access to the key using an IAM policy. Adding the user to a bucket policy can grant the user permission to access encrypted objects in the bucket but doesn't necessarily give the user the ability to decrypt those objects.",
    "6. You run a public-facing application on EC2 instances. The application is backed by a database running on RDS. Users access it using multiple domain names that are hosted in Route 53. You want to get an idea of what IP addresses are accessing your application. Which of the following would you stream to CloudWatch Logs to get this information?\n   1. RDS logs\n   2. DNS query logs\n   3. VPC flow logs\n   4. CloudTrail logs": "3. VPC flow logs record source IP address information for traffic coming into your VPC. DNS query logs record the IP addresses of DNS queries, but those won't necessarily be the same IP addresses accessing your application. Because users won't directly connect to your RDS instance, RDS logs won't record their IP addresses. CloudTrail logs can record the source IP address of API requests but not connections to an EC2 instance.",
    "7. You're running a web server that keeps a detailed log of web requests. You want to determine which IP address has made the most requests in the last 24 hours. What should you do to accomplish this? (Choose two.)\n   1. Create a metric filter.\n   2. Stream the web server logs to CloudWatch Logs.\n   3. Upload the web server log to S3.\n   4. Use Athena to query the data.": "3, 4. Athena lets you perform advanced SQL queries against data stored in S3. A metric filter can increment based on the occurrence of a value in a CloudWatch log group but can't tell you the most frequently occurring IP address.",
    "8. An application running on an EC2 instance has been updated to send large amounts of data to a server in your datacenter for backup. Previously, the instance generated very little traffic. Which GuardDuty finding type is this likely to trigger?\n   1. Behavior\n   2. Backdoor\n   3. Stealth\n   4. ResourceConsumption": "1. The Behavior finding type is triggered by an instance sending abnormally large amounts of data or communicating on a protocol and port that it typically doesn't. The Backdoor finding type indicates that an instance has resolved a DNS name associated with a command-and-control server or is communicating on TCP port 25. The Stealth finding type is triggered by weakening password policies or modifying a CloudTrail configuration. The ResourceConsumption finding type is triggered when an IAM user launches an EC2 instance when they've never done so.",
    "9. You've set up an AWS Config managed rule to check whether a particular security group is attached to every instance in a VPC. You receive an SNS notification that an instance is out of compliance. But when you check the instance a few hours later, the security group is attached. Which of the following may help explain the apparent discrepancy? (Choose two.)\n   1. The AWS Config timeline\n   2. Lambda logs\n   3. CloudTrail management event logs\n   4. VPC flow logs": "1, 3. The AWS Config timeline will show every configuration change that occurred on the instance, including the attachment and detachment of security groups. CloudTrail management event logs will also show the actions that detached and attached the security group. Although AWS Config rules use Lambda functions, the Lambda logs for AWS managed rules are not available to you. VPC flow logs capture traffic ingressing a VPC, but not API events.",
    "10. You want to use Amazon Inspector to analyze the security posture of your EC2 instances running Windows. Which rules package should you not use in your assessment?\n   1. Common Vulnerabilities and Exposures\n   2. Center for Internet Security Benchmarks\n   3. Runtime Behavior Analysis\n   4. Security Best Practices": "4. The Security Best Practices rules package has rules that apply to only Linux instances. The other rules contain rules for both Windows and Linux instances.",
    "11. You have a distributed application running in datacenters around the world. The application connects to a public Simple Queue Service (SQS) endpoint to send messages to a queue. How can you prevent an attacker from using this endpoint to gain unauthorized access to the queue? (Choose two.)\n   1. Network access control lists\n   2. Security groups\n   3. IAM policies\n   4. SQS access policies": "3, 4. You can use an IAM policy or SQS access policy to restrict queue access to certain principals or those coming from a specified IP range. You cannot use network access control lists or security groups to restrict access to a public endpoint.",
    "12. You're using a public-facing application load balancer to forward traffic to EC2 instances in an Auto Scaling group. What can you do to ensure users on the Internet can reach the load balancer over HTTPS without reaching your instances directly? (Choose two.)\n   1. Create a security group that allows all inbound traffic to TCP port 443.\n   2. Attach the security group to the instances.\n   3. Attach the security group to the load balancer.\n   4. Remove the Internet gateway from the VPC.\n   5. Create a security group that allows all inbound traffic to TCP port 80.": "1, 3. HTTPS traffic traverses TCP port 443, so the security group should allow inbound access to this protocol and port. HTTP traffic uses TCP port 80. Because users need to reach the ALB but not the instances directly, the security group should be attached to the ALB. Removing the Internet gateway would prevent users from reaching the ALB as well as the EC2 instances directly.",
    "13. You're running a UDP-based application on an EC2 instance. How can you protect it against a DDoS attack?\n   1. Place the instance behind a network load balancer.\n   2. Implement a security group to restrict inbound access to the instance.\n   3. Place the instance behind an application load balancer.\n   4. Enable AWS Shield Standard.": "2. A security group to restrict inbound access to authorized sources is sufficient to guard against a UDP-based DDoS attack. Elastic load balancers do not provide UDP listeners, only TCP. AWS Shield is enabled by default and protects against those UDP-based attacks from sources that are allowed by the security group.",
    "14. You're running a web application on six EC2 instances behind a network load balancer. The web application uses a MySQL database. How can you protect your application against SQL injection attacks? (Choose two.)\n   1. Enable WAF.\n   2. Assign elastic IP addresses to the instances.\n   3. Place the instances behind an application load balancer.\n   4. Block TCP port 3306.": "1, 3. WAF can block SQL injection attacks against your application, but only if it's behind an application load balancer. It's not necessary for the EC2 instances to have an elastic IP address. Blocking access to TCP port 3306, which is the port that MySQL listens on for database connections, may prevent direct access to the database server but won't prevent a SQL injection attack.",
    "15. Which services protect against an HTTP flood attack?\n   1. GuardDuty\n   2. WAF\n   3. Shield Standard\n   4. Shield Advanced": "2, 4. Both WAF and Shield Advanced can protect against HTTP flood attacks, which are marked by excessive or malformed requests. Shield Advanced includes WAF at no charge. Shield Standard does not offer protection against Layer 7 attacks. GuardDuty looks for signs of an attack but does not prevent one.",
    "16. Your security policy requires that you use a KMS key for encrypting S3 objects. It further requires this key be rotated once a year and revoked when misuse is detected. Which key type should you use? (Choose two.)\n   1. Customer-managed CMK\n   2. AWS-managed CMK\n   3. S3-managed key\n   4. Customer-provided key": "1, 4. You can revoke and rotate both a customer-managed CMK and a customer-provided key at will. You can't revoke or rotate an AWS-managed CMK or an S3-managed key.",
    "17. A developer is designing an application to run on AWS and has asked for your input in deciding whether to use a SQL database or DynamoDB for storing highly transactional application data. Your security policy requires all application data to be encrypted and encryption keys to be rotated every 90 days. Which AWS service should you recommend for storing application data? (Choose two.)\n   1. KMS\n   2. RedShift\n   3. DynamoDB\n   4. RDS": "3, 4. Customer-managed Customer Master Keys (CMKs) can be rotated at will, whereas AWS-managed CMKs are rotated only once a year. RDS and DynamoDB let you use a customer-managed CMK to encrypt data. RedShift is not designed for highly transactional databases and is not appropriate for the application. KMS stores and manages encryption keys but doesn't store application data.",
    "18. You need to copy the data from an unencrypted EBS volume to another region and encrypt it. How can you accomplish this? (Choose two.)\n   1. Create an encrypted snapshot of the unencrypted volume.\n   2. Simultaneously encrypt and copy the snapshot to the destination region.\n   3. Copy the encrypted snapshot to the destination region.\n   4. Create an unencrypted snapshot of the unencrypted volume.": "2, 4. To encrypt data on an unencrypted EBS volume, you must first take a snapshot. The snapshot will inherit the encryption characteristics of the source volume, so an unencrypted EBS volume will always yield an unencrypted snapshot. You can then simultaneously encrypt the snapshot as you copy it to another region.",
    "19. An instance with an unencrypted EBS volume has an unencrypted EFS filesystem mounted on it. You need to encrypt the data on an existing EFS filesystem using a KMS key. How can you accomplish this?\n   1. Encrypt the EBS volume of the instance.\n   2. Create a new encrypted EFS filesystem and copy the data to it.\n   3. Enable encryption on the existing EFS filesystem.\n   4. Use a third-party encryption program to encrypt the data.": "2. You can enable encryption on an EFS filesystem only when you create it; therefore, the only option to encrypt the data using KMS is to create a new EFS filesystem and copy the data to it. A third-party encryption program can't use KMS keys to encrypt data. Encrypting the EBS volume will encrypt the data stored on the volume, but not on the EFS filesystem.",
    "20. On which of the following can you not use an ACM-generated TLS certificate? (Choose two.)\n   1. An S3 bucket\n   2. A CloudFront distribution\n   3. An application load balancer\n   4. An EC2 instance": "1, 4. You can install an ACM-generated certificate on a CloudFront distribution or application load balancer. You can't export the private key of an ACM-generated certificate, so you can't install it on an EC2 instance. AWS manages the TLS certificates used by S3.",
    "21. Which of the following assesses the security posture of your AWS resources against AWS best practices?\n   1. Detective\n   2. Macie\n   3. Security Hub\n   4. GuardDuty": "3. Security Hub checks the configuration of your AWS services against AWS best practices.",
    "1. Which of the following best describes the AWS Free Tier?\n   1. Free access to AWS services for a new account's first month\n   2. Free access to all instance types of AWS EC2 instances for new accounts\n   3. Free access to basic levels of AWS services for a new account's first year\n   4. Unlimited and open-ended access to the \u201cfree tier\u201d of most AWS services": "3. The Free Tier provides free access to basic levels of AWS services for a new account's first year.",
    "2. Which of the following storage classes provides the least expensive storage and transfer rates?\n   1. Amazon S3 Glacier\n   2. Amazon S3 Standard-Infrequent Access\n   3. Amazon S3 Standard\n   4. Amazon S3 One Zone-Infrequent Access": "1. Standard provides the most replicated and quickest-access service and is, therefore, the most expensive option. Storage rates for Standard-Infrequent and One Zone-Infrequent are lower than Standard but are still more expensive than Glacier.",
    "3. Which AWS service is best suited to controlling your spending by sending email alerts?\n   1. Cost Explorer\n   2. Budgets\n   3. Organizations\n   4. TCO Calculator": "2. Cost Explorer provides usage and spending data. Organizations lets you combine multiple AWS accounts under a single administration. TCO Calculator would let you compare the costs of running an application on AWS versus locally, but it no longer exists.",
    "4. Your AWS infrastructure is growing and you're beginning to have trouble keeping track of what you're spending. Which AWS service is best suited to analyzing account usage data at scale?\n   1. Trusted Advisor\n   2. Cost Explorer\n   3. Budgets\n   4. Cost and Usage Reports": "4. Cost Explorer provides usage and spending data, but without the ability to easily incorporate Redshift and QuickSight that Cost and Usage Reports offers. Trusted Advisor checks your account for best-practice compliance. Budgets allows you to set alerts for problematic usage.",
    "5. Your company wants to more closely coordinate the administration of its multiple AWS accounts, and AWS Organizations can help it do that. How does that change your security profile? (Choose three.)\n   1. An organization-level administration account breach is potentially more damaging.\n   2. User permissions can be controlled centrally at the organization level.\n   3. You should upgrade to use only specially hardened organization-level VPCs.\n   4. Standard security best practices such as MFA and strong passwords are even more essential.\n   5. You should upgrade all of your existing security groups to account for the changes.": "1, 2, 4. As efficient as Organizations can be, so does the threat they represent grow. There is no such thing as a specially hardened organization-level VPC. Security groups don't require any special configuration.",
    "6. Which of the following resource states are monitored by AWS Trusted Advisor? (Choose two.)\n   1. Route 53 routing failures\n   2. Running but idle EC2 instances\n   3. S3 buckets with public read access permissions\n   4. EC2 Linux instances that allow root account SSH access\n   5. Unencrypted S3 bucket data transfers": "2, 3. Trusted Advisor monitors your EC2 instances for lower than 10 percent CPU and network I/O below 5 MB on four or more days. Trusted Advisor doesn't monitor Route 53 hosted zones or the status of S3 data transfers. Proper OS-level configuration of your EC2 instances is your responsibility.",
    "7. You're planning a new AWS deployment, and your team is debating whether they'll be better off using an RDS database or one run on an EC2 instance. Which of the following tools will be most helpful?\n   1. TCO Calculator\n   2. AWS Pricing Calculator\n   3. Trusted Advisor\n   4. Cost and Usage Reports": "2. The Pricing Calculator is the most direct tool for this kind of calculation. TCO Calculator helps you compare costs of on-premises to AWS deployments. Trusted Advisor checks your account for best-practice compliance. Cost and Usage Reports help you analyze data from an existing deployment.",
    "8. Which of the following is not a metric you can configure an AWS budget to track?\n   1. EBS volume capacity\n   2. Resource usage\n   3. Reserve instance coverage\n   4. Resource cost": "1. Monitoring of EBS volumes for capacity is not within the scope of budgets.",
    "9. Which of the following statements are true of cost allocation tags? (Choose two.)\n   1. Tags can take up to 24 hours before they appear in the Billing and Cost Management dashboard.\n   2. Tags can't be applied to resources that were launched before the tags were created.\n   3. You're allowed five free budgets per account.\n   4. You can activate and manage cost allocation tags from the Tag Editor page.": "1, 2. Tags can take up to 24 hours to appear and they can't be applied to legacy resources. You're actually allowed only two free budgets per account. Cost allocation tags are managed from the Cost Allocation Tags page.",
    "10. Your online web store normally requires three EC2 instances to handle traffic but experiences a twofold increase in traffic for the two summer months. Which of the following approaches makes the most sense?\n   1. Run three on-demand instances 12 months per year and schedule six reserve instances for the summer months.\n   2. Run three spot instances for the summer months and three reserve instances 12 months/year.\n   3. Run nine reserve instances for 12 months/year.\n   4. Run three reserve instances 12 months/year and purchase three scheduled reserve instances for the summer months.": "4. The most effective approach would be to run three reserve instances 12 months/year and purchase three scheduled reserve instances for the summer. Spot instances are not appropriate because they shut down automatically. Since it's possible to schedule an RI to launch within a recurring block of time, provisioning other instance configurations for the summer months will be wasteful.",
    "11. Which of the following settings do you not need to provide when configuring a reserved instance?\n   1. Payment option\n   2. Standard or Convertible RI\n   3. Interruption policy\n   4. Tenancy": "3. Interruption polices are relevant to spot instances, not reserved instances. Payment options (All Upfront, Partial Upfront, or No Upfront), reservation types (Standard or Convertible RI), and tenancy (Default or Dedicated) are all necessary settings for RIs.",
    "12. Your new web application requires multiple EC2 instances running 24/7 and you're going to purchase reserved instances. Which of the following payment options is the most expensive when configuring a reserve instance?\n   1. All Upfront\n   2. Partial Upfront\n   3. No Upfront\n   4. Monthly": "3. No Upfront is the most expensive option. The more you pay up front, the lower the overall cost. There's no option called Monthly.",
    "13. Which of the following benefits of containers such as Docker can significantly reduce your AWS compute costs? (Choose two.)\n   1. Containers can launch quickly.\n   2. Containers can deliver increased server density.\n   3. Containers make it easy to reliably replicate server environments.\n   4. Containers can run using less memory than physical machines.": "2, 4. Containers are more dense and lightweight. Containers do tend to launch more quickly than EC2 instances and do make it easy to replicate server environments, but those are not primarily cost savings.",
    "14. Which of the following is the best use of an EC2 reserved instance?\n   1. An application that will run continuously for six months straight\n   2. An application that will run continuously for 36 months straight\n   3. An application that runs only during local business hours\n   4. An application that runs at unpredictable times and can survive unexpected shutdowns": "2. Standard reserve instances make the most sense when they need to be available 24/7 for at least a full year, with even greater savings over three years. Irregular or partial workdays are not good candidates for this pricing model.",
    "15. Which of the following describes \u201cunused EC2 instances matching a particular set of launch specifications\u201d?\n   1. Request type\n   2. Spot instance interruption\n   3. Spot fleet\n   4. Spot instance pool": "4. A spot instance pool is made up of unused EC2 instances. There are three request types: Request, Request And Maintain, and Reserve For Duration. A spot instance interruption occurs when the spot price rises above your maximum. A spot fleet is a group of spot instances launched together.",
    "16. Which of the following best describes a spot instance interruption?\n   1. A spot instance interruption occurs when the spot price rises above your maximum.\n   2. A spot instance interruption is the termination of a spot instance when its workload completes.\n   3. A spot instance interruption occurs when a spot request is manually restarted.\n   4. A spot instance interruption is the result of a temporary outage in an AWS datacenter.": "1. A spot instance interruption occurs when the spot price rises above your maximum. Workload completions and datacenter outages are never referred to as interruptions. Spot requests can't be manually restarted.",
    "17. Which of the following describes the maximum instances or vCPUs you want running?\n   1. Spot instance pool\n   2. Target capacity\n   3. Spot maximum\n   4. Spot cap": "2. Target capacity represents the maximum instances you want running. A spot instance pool contains unused EC2 instances matching a particular set of launch specifications. Spot maximum and spot cap sound good but aren't terms normally used in this context.",
    "18. You need to make sure your EBS volumes are regularly backed up, but you're afraid you'll forget to remove older snapshot versions, leading to expensive data bloat. What's the best solution to this problem?\n   1. Configure the EBS Lifecycle Manager.\n   2. Create a script that will regularly invoke the AWS CLI to prune older snapshots.\n   3. Configure an EBS Scheduled Reserved Instance.\n   4. Tie a string to your finger.\n   5. Configure an S3 Lifecycle configuration policy to remove old snapshots.": "1. The EBS Lifecycle Manager can be configured to remove older EBS snapshots according to your needs. Creating a script is possible, but it's nowhere near as simple, and it's not tightly integrated with your AWS infrastructure. There is no \u201cEBS Scheduled Reserve Instance\u201d but there is an \u201cEC2 Scheduled Reserve Instance.\u201d Tying a string? Really? EBS snapshots are stored in S3, but you can't access the buckets that they're kept in.",
    "19. Which of these AWS CLI commands will launch a spot fleet?\n   1. aws ec2 request-fleet --spot-fleet-request-config file://Config.json\n   2. aws ec2 spot-fleet --spot-fleet-request-config file://Config.json\n   3. aws ec2 launch-spot-fleet --spot-fleet-request-config/ file://Config.json\n   4. aws ec2 request-spot-fleet --spot-fleet-request-config/ file://Config.json": "4. The command is request-spot-fleet. The --spot-fleet-request-config argument points to a JSON configuration file.",
    "20. Which of the following elements is not something you'd include in your spot fleet request?\n   1. Availability zone\n   2. Target capacity\n   3. Platform (the instance OS)\n   4. AMI": "3. The availability zone, target capacity, and AMI are all elements of a complete spot fleet request."
}